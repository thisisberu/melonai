{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Standard library imports\n",
    "import os\n",
    "import ast \n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, List, Optional, Literal, Annotated, Sequence\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, Markdown\n",
    "import asyncio\n",
    "from rich import print\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd \n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from rich import print\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Union, Optional, Annotated, Literal, Sequence\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine_v1 as discoveryengine\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from typing import Dict\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from google.cloud import translate_v3\n",
    "import langid\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "LOCATION = (\n",
    "    \"global\"  # The location for the Translation API (use a specific region if needed)\n",
    ")\n",
    "\n",
    "\n",
    "# Base models\n",
    "class ImageInput(BaseModel):\n",
    "    \"\"\"Represents an uploaded image.\"\"\"\n",
    "\n",
    "    filename: str\n",
    "\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    \"\"\"Represents user input, which can include text and/or an image.\"\"\"\n",
    "\n",
    "    text: str\n",
    "    image: Optional[ImageInput] = None\n",
    "\n",
    "\n",
    "class AgentState(Dict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    retrieved_docs: str\n",
    "    user_input: UserInput\n",
    "    corrected_question: Optional[str] = None\n",
    "    has_image: bool = False\n",
    "    image_path: Optional[str] = None\n",
    "    user_id: str\n",
    "    retrieved_products: str\n",
    "    intents: List[\n",
    "        str\n",
    "    ] = []  # Can include: \"health\", \"meal_plate\", \"product_recommendation\"\n",
    "    agent_responses: Dict[str, str] = {}  # Store responses from different agents\n",
    "    active_agents: List[str] = []\n",
    "    intent_queries: Dict[str, List[str]] = {}  # Store sub-queries for each intent\n",
    "    final_answer: Dict[str,List[str]] = {} \n",
    "\n",
    "class EnhancedConversationManager:\n",
    "    def __init__(self, project_id: str, data_stores: List[Dict[str, str]]):\n",
    "        \"\"\"Initialize with multiple data stores.\"\"\"\n",
    "        self.project_id = project_id\n",
    "        self.data_stores = data_stores\n",
    "        self.session_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.start_time = datetime.now().isoformat()\n",
    "        self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.current_context = []\n",
    "        self.used_sources = []\n",
    "        # Initialize question history as instance variable\n",
    "        self.question_history = []\n",
    "        self.answer_history = []\n",
    "        self.conversation_history = []\n",
    "        self.language = \"\"\n",
    "        self.conditions = []\n",
    "        self.products = \"\" \n",
    "        # Single initialization of shared components\n",
    "        self._initialize_shared_components()\n",
    "        # Initialize workflow once\n",
    "        self.initialize_workflow()\n",
    "\n",
    "    def _initialize_shared_components(self):\n",
    "        \"\"\"Initialize API clients and conversations for all data stores at once.\"\"\"\n",
    "        print(\"Starting unified initialization\")\n",
    "        self.clients = {}\n",
    "        self.conversations = {}\n",
    "\n",
    "        # Group data stores by location to minimize client creation\n",
    "        location_grouped_stores = {}\n",
    "        for store in self.data_stores:\n",
    "            location = store[\"location\"]\n",
    "            if location not in location_grouped_stores:\n",
    "                location_grouped_stores[location] = []\n",
    "            location_grouped_stores[location].append(store)\n",
    "\n",
    "        # Create one client per location\n",
    "        for location, stores in location_grouped_stores.items():\n",
    "            client_options = (\n",
    "                ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "                if location != \"global\"\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            client = discoveryengine.ConversationalSearchServiceClient(\n",
    "                client_options=client_options\n",
    "            )\n",
    "\n",
    "            # Create conversations for all stores in this location\n",
    "            for store in stores:\n",
    "                store_id = store[\"data_store_id\"]\n",
    "                self.clients[store_id] = client\n",
    "\n",
    "                # Create conversation for this store\n",
    "                conversation = client.create_conversation(\n",
    "                    parent=client.data_store_path(\n",
    "                        project=self.project_id,\n",
    "                        location=store[\"location\"],\n",
    "                        data_store=store_id,\n",
    "                    ),\n",
    "                    conversation=discoveryengine.Conversation(),\n",
    "                )\n",
    "                self.conversations[store_id] = conversation\n",
    "\n",
    "        print(\"Completed unified initialization\")\n",
    "\n",
    "    def initialize_workflow(self):\n",
    "        # Initialize workflow\n",
    "        self.workflow = StateGraph(AgentState)\n",
    "\n",
    "        # Add nodes\n",
    "        self.workflow.add_node(\"process_input\", process_input)\n",
    "        self.workflow.add_node(\"initial_agent\", initial_agent)\n",
    "        self.workflow.add_node(\"intent_agent\", intent_agent)\n",
    "        self.workflow.add_node(\"process_parallel\", process_parallel)\n",
    "        self.workflow.add_node(\"generate\", generate)\n",
    "        # self.workflow.add_node(\"retrieve\",retrieve)\n",
    "        # self.workflow.add_node(\"meal_plate\", analyze_meal_question)\n",
    "        # self.workflow.add_node(\"product_recommendation\", recommend_products)\n",
    "\n",
    "        # Define edges\n",
    "        self.workflow.set_entry_point(\"process_input\")\n",
    "        self.workflow.add_edge(\"process_input\", \"initial_agent\")\n",
    "        self.workflow.add_edge(\"initial_agent\", \"intent_agent\")\n",
    "        self.workflow.add_edge(\"intent_agent\", \"process_parallel\")\n",
    "        self.workflow.add_edge(\"process_parallel\", \"generate\")\n",
    "\n",
    "        self.workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        # Compile graph\n",
    "        self.graph = self.workflow.compile()\n",
    "        # display(Image(self.graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "        # Display graph visualization\n",
    "        from IPython.display import Image, display\n",
    "        graph_image = self.graph.get_graph().draw_mermaid_png()\n",
    "        display(Image(graph_image))\n",
    "\n",
    "    def add_question_to_history(self, question: str):\n",
    "        \"\"\"Add a new question to the history.\"\"\"\n",
    "        self.question_history.append(question)\n",
    "\n",
    "    def get_recent_questions(self, limit: int = 1) -> List[str]:\n",
    "        \"\"\"Get the most recent questions, default last 20.\"\"\"\n",
    "        return self.question_history[-limit:]\n",
    "\n",
    "    def process_question(self, user_input: UserInput, user_id: str):\n",
    "        \"\"\"Process a question with optional image input.\"\"\"\n",
    "\n",
    "        \n",
    "        inputs = {\n",
    "            \"user_input\": user_input,\n",
    "            \"image_processed\": False,  # Add flag to track image processing\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "\n",
    "        # Process through workflow\n",
    "        final_response = \"\"\n",
    "        for output in self.graph.stream(inputs):\n",
    "            for key, value in output.items():\n",
    "                if isinstance(value, dict) and \"messages\" in value:\n",
    "                    messages = value[\"messages\"]\n",
    "                    if messages:\n",
    "                        final_response = messages[-1].content\n",
    "                        # final_answer = messages[-1]\n",
    "        \n",
    "        return final_response\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def get_token(auth_url, client_id, client_secret):\n",
    "    auth_data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_secret\": client_secret,\n",
    "        \"client_id\": client_id,\n",
    "    }\n",
    "    response = requests.post(auth_url, json=auth_data)\n",
    "    print(\"-----response from get token----\",response.json())\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"access_token\")\n",
    "\n",
    "\n",
    "def evaluate_api_response(state, api_response: Dict, user_question: str) -> str:\n",
    "    print(\"-----api_response-----\",api_response) \n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "\n",
    "    # Split the analysis into two prompts for more focused, concise responses\n",
    "    question_prompt = PromptTemplate(\n",
    "        template=\"\"\"Answer the user's question based on this meal analysis: {meal_analysis}. \n",
    "\n",
    "User Question: {user_question}\n",
    "\n",
    "Provide a concise, conversational response that directly addresses their question. Keep it brief.\"\"\",\n",
    "        input_variables=[\"meal_analysis\", \"user_question\"],\n",
    "    )\n",
    "\n",
    "    gut_health_prompt = PromptTemplate(\n",
    "        template=\"\"\"Based on your previous answer, analyze the gut health impact of the mentioned foods using these categories:\n",
    "GUT-POSITIVE FOODS (Should make up 60% of daily calories):\n",
    "        - Vegetables and greens: spinach, lettuce, broccoli, cauliflower, onions, asparagus, kale, tomatoes, squash\n",
    "        - Fruits and berries: apples, oranges, strawberries, kiwis, blueberries, pears, peaches, bananas\n",
    "        - Legumes, nuts and seeds: lentils, chickpeas, beans, pumpkin seeds, peanuts, sunflower seeds\n",
    "        - Whole grain pasta, bread, cereal and tortillas\n",
    "        - Lean protein: chicken breast, turkey, tofu, lean ground beef, eggs, salmon, tuna, trout, shrimp\n",
    "        - Cultured and fermented foods: kimchi, sauerkraut, kefir, yogurt with live bacteria\n",
    "        - Healthy fats: olive oil, avocados\n",
    "        - Herbs and spices\n",
    "        - Hydrating beverages: water, herbal tea\n",
    " \n",
    "        GUT-NEUTRAL FOODS (Up to 25% of daily calories):\n",
    "        - Caffeine (limit to 2 cups of coffee per day)\n",
    "        - Red meats (limit to 2 servings per week)\n",
    "        - Sweetened and whole dairy: sour cream, cheese, butter\n",
    "        - Foods with high natural sugar content: honey, grapes, overripe bananas\n",
    " \n",
    "        GUT-NEGATIVE FOODS (Should be limited to 15% of daily calories):\n",
    "        - Fast foods: fries, burgers, chips, convenience meals, palm oil\n",
    "        - Processed and fatty meat: bacon, ham, deli meats, salami, sausage, hot dogs, lamb, steak, pork\n",
    "        - Refined carbs: white bread, biscuits, white pasta, tortillas, white rice, corn products\n",
    "        - Sugary and artificially sweetened beverages\n",
    "        - Products with added sugar: candies, cookies, cakes, pastries, ice cream\n",
    "        - Alcohol\n",
    "\n",
    "Previous Response: {previous_response}\n",
    "\n",
    "Provide a brief, friendly analysis that:\n",
    "1. Now identify which category (gut-positive, neutral, or negative) each food item falls into.Keep it brief. \n",
    "2. If any gut-negative foods are identified, suggest healthier gut-positive alternatives that are similar in nature or can satisfy the same craving.\n",
    "        For example: If pizza is mentioned (gut-negative due to refined carbs), suggest alternatives like:\n",
    "        - Whole grain pizza crust with vegetable toppings\n",
    "        - Cauliflower crust pizza with lean protein\n",
    "        Only suggest alternatives if there are reasonable gut-positive substitutions available. Keep it brief.\n",
    "\n",
    "Keep it conversational, like you're chatting with a friend. Use emoji sparingly if appropriate.\"\"\",\n",
    "        input_variables=[\"previous_response\"],\n",
    "    )\n",
    "\n",
    "    # Create two separate chains\n",
    "    question_chain = question_prompt | model | StrOutputParser()\n",
    "    gut_health_chain = gut_health_prompt | model | StrOutputParser()\n",
    "\n",
    "    # Execute chains sequentially\n",
    "    initial_response = question_chain.invoke(\n",
    "        {\"meal_analysis\": str(api_response), \"user_question\": user_question}\n",
    "    )\n",
    "\n",
    "    gut_health_response = gut_health_chain.invoke(\n",
    "        {\"previous_response\": initial_response}\n",
    "    )\n",
    "\n",
    "    # Combine responses with appropriate spacing\n",
    "    final_response = f\"{initial_response}\\n\\n{gut_health_response}\"\n",
    "\n",
    "    manager.answer_history.append(final_response)\n",
    "    return final_response\n",
    "\n",
    "\n",
    "def translate_text(state, text, language_code) -> translate_v3.TranslationServiceClient:\n",
    "    client = translate_v3.TranslationServiceClient()\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{LOCATION}\"\n",
    "    print(parent)\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "    # Translate text from English to chosen language\n",
    "    # Supported mime types: # https://cloud.google.com/translate/docs/supported-formats\n",
    "    response = client.translate_text(\n",
    "        contents=[text],\n",
    "        target_language_code=language_code,\n",
    "        parent=parent,\n",
    "        mime_type=\"text/plain\",\n",
    "    )\n",
    "    current_translated_output = \"\"\n",
    "    # Display the translation for each input text provided\n",
    "    for translation in response.translations:\n",
    "        # print(f\"Translated text: {translation.translated_text}\")\n",
    "        manager.language = translation.detected_language_code\n",
    "        current_translated_output += translation.translated_text\n",
    "\n",
    "    return current_translated_output\n",
    "\n",
    "\n",
    "# Main workflow nodes\n",
    "def process_input(state):\n",
    "    \"\"\"Process the initial user input with improved image handling.\"\"\"\n",
    "    print(\"\\n---PROCESS INPUT---\")\n",
    "    user_input = state[\"user_input\"]\n",
    "    print(f\"Raw input text: {user_input.text}\")\n",
    "\n",
    "    if user_input.image and not state.get(\"image_processed\", False):\n",
    "        print(f\"Image attached: {user_input.image.filename}\")\n",
    "        content = f\"{user_input.text}\\n[Attached image: {user_input.image.filename}]\"\n",
    "        state[\"image_processed\"] = True\n",
    "    else:\n",
    "        content = user_input.text\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=content)],\n",
    "        \"user_input\": user_input,\n",
    "        \"image_processed\": state.get(\"image_processed\", False),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def initial_agent(state: Dict) -> Dict:\n",
    "    print(\"\\n---INITIAL AGENT---\")\n",
    "    current_question = state[\"user_input\"].text\n",
    "    print(f\"Input question: {current_question}\")\n",
    "\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "\n",
    "    cur_lang = langid.classify(current_question)[0]\n",
    "\n",
    "    if cur_lang == \"en\":\n",
    "        print(\"Its english already\")\n",
    "        manager.language = \"en\"\n",
    "    else:\n",
    "        current_question = translate_text(state, current_question, \"en\")\n",
    "        print(\"translated_question: \", current_question)\n",
    "\n",
    "    has_image = state[\"user_input\"].image is not None\n",
    "    if has_image:\n",
    "        manager.add_question_to_history(current_question)\n",
    "        return state\n",
    "    try:\n",
    "        if \"error\" in state:\n",
    "            return state\n",
    "\n",
    "        # Initialize the conversation memory with a window size of k=5\n",
    "        memory = ConversationBufferWindowMemory(\n",
    "            memory_key=\"chat_history\", k=1, return_messages=True\n",
    "        )\n",
    "\n",
    "        # Add recent questions to memory\n",
    "        recent_questions = manager.get_recent_questions()\n",
    "        reversed_questions = list(reversed(recent_questions))\n",
    "\n",
    "        # Add recent answers to memory\n",
    "        recent_answers = manager.answer_history[-1:]\n",
    "        reversed_answers = list(reversed(recent_answers))\n",
    "\n",
    "        # Save context for past questions and answers in memory\n",
    "        for question, answer in zip(reversed_questions, reversed_answers):\n",
    "            memory.save_context(\n",
    "                {\"input\": question}, {\"output\": answer}\n",
    "            )  # Save both question and answer\n",
    "\n",
    "        print(\"inside memory \", memory)\n",
    "\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"user_input\", \"chat_history\"],\n",
    "            template=(\n",
    "                \"You are an intelligent assistant designed to process user questions with precise context awareness. \"\n",
    "                \"IMPORTANT CONTEXT: 'ABO' or 'ABOs' refers to Amway Business Owner(s). Never change or reinterpret these terms.\\n\\n\"\n",
    "                \"Follow these steps:\\n\\n\"\n",
    "                \"1. First, analyze if the user's question is standalone:\\n\"\n",
    "                \"   - Can it be fully understood without any context?\\n\"\n",
    "                \"   - Does it contain all necessary information?\\n\"\n",
    "                \"   - Is it grammatically complete?\\n\\n\"\n",
    "                \"2. If YES to all above (standalone question):\\n\"\n",
    "                \"   - Return the question exactly as provided\\n\"\n",
    "                \"   - DO NOT add any context from chat history\\n\"\n",
    "                \"   - Only fix obvious grammatical errors if any\\n\\n\"\n",
    "                \"3. If NO to any above (context-dependent question):\\n\"\n",
    "                \"   - Reference ONLY the immediately preceding interaction:\\n\"\n",
    "                \"   Chat history: {chat_history}\\n\"\n",
    "                \"   - Add minimal necessary context to make the question clear\\n\"\n",
    "                \"   - Maintain the user's original intent and tone\\n\"\n",
    "                \"   - Focus on pronouns, references, and implicit subjects\\n\\n\"\n",
    "                \"User's question: {user_input}\\n\\n\"\n",
    "                \"Refined question:\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Set up the language model with a specified temperature and model type\n",
    "        model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "        \n",
    "        # chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "        \n",
    "        chain = prompt_template | model\n",
    "        \n",
    "        # Load conversation history from memory\n",
    "        chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "        # Invoke the chain to generate a refined question\n",
    "        result = chain.invoke(\n",
    "            {\n",
    "                \"user_input\": current_question,\n",
    "                \"chat_history\": chat_history,  # Pass the loaded chat history\n",
    "            }\n",
    "        )\n",
    "        # Store the corrected question in the state - handle AIMessage result\n",
    "        state[\"corrected_question\"] = result.content\n",
    "        \n",
    "        # Log the current refined question to history\n",
    "        manager.add_question_to_history(result.content)\n",
    "\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in initial_agent: {str(e)}\")\n",
    "        state[\"error\"] = str(e)\n",
    "        return state\n",
    "\n",
    "# Add new intent detection function\n",
    "def detect_intent(question: str, has_image: bool) -> List[str]:\n",
    "    \"\"\"Determine which agents should handle the question.\"\"\"\n",
    "    \n",
    "    # question = state[\"corrected_question\"]\n",
    "    # has_image = state[\"has_image\"]\n",
    "    class IntentClassification(BaseModel):\n",
    "        \"\"\"Classification of user question intents.\"\"\"\n",
    "\n",
    "        health_intent: bool = Field(\n",
    "            description=\"Question relates to health, nutrition,wellness advice or releated to diseases\"\n",
    "        )\n",
    "        meal_plate_intent: bool = Field(\n",
    "            description=\"Question about specific meal, food item, or plate\"\n",
    "        )\n",
    "        product_intent: bool = Field(\n",
    "            description=\"Question asking for product recommendations\"\n",
    "        )\n",
    "        incorrect_question: bool = Field(\n",
    "            description=\"Question is not about health, nutrition, wellness,related to diseases, meal plate, or product recommendation\"\n",
    "        )\n",
    "        reasoning: str = Field(description=\"Explanation for the classification\")\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    llm_with_tool = model.with_structured_output(IntentClassification)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Analyze the following question and determine its intents. A question can have multiple intents.\n",
    "\n",
    "Question: {question}\n",
    "Image Present: {has_image}\n",
    "\n",
    "Determine if the question matches any of these intents:\n",
    "\n",
    "1. MEAL PLATE INTENT (especially important if image present):\n",
    "    (Priority over health intent if about specific food):\n",
    "    ANY question about a specific food item or meal should ONLY be classified as meal_plate_intent, NOT health_intent.\n",
    "    This includes:\n",
    "    Determine if the question is about a CONCRETE meal, food plate, or named food item.\n",
    "    It should be marked as a meal question if ANY of these conditions are met:\n",
    " \n",
    "    1. EXPLICIT FOOD REFERENCES:\n",
    "       - Question asks about a specific, concrete meal or food item\n",
    "       - Examples: \"What's in this sandwich?\", \"Is this pasta healthy?\", \"How many calories in this apple?\"\n",
    " \n",
    "    2. IMAGE CONTEXT INDICATORS:\n",
    "       - Question uses demonstrative pronouns (\"this\", \"that\") implying something visible\n",
    "       - Question seeks judgment about edibility or quality (\"good to eat\", \"safe to eat\", \"looks okay\")\n",
    "       - Question asks about appearance or condition (\"does this look right\", \"is this done\")\n",
    "       - The user_input indicates an image was uploaded\n",
    "       Examples:\n",
    "       - \"Is this good to eat?\"\n",
    "       - \"Does this look right?\"\n",
    "       - \"Can I eat this?\"\n",
    " \n",
    "    3. IMMEDIATE FOOD DECISIONS:\n",
    "       - Question asks for judgment about consuming specific food\n",
    "       - Examples: \"Should I eat this?\", \"Is this safe?\", \"Good to consume?\"\n",
    " \n",
    "    Do NOT mark as a meal question if it's about:\n",
    "    1. General meal concepts or timing (e.g., \"When should I eat breakfast?\")\n",
    "    2. General meal types or categories (e.g., \"What is a balanced breakfast?\")\n",
    "    3. Dietary patterns or habits (e.g., \"Should I skip breakfast?\")\n",
    "    4. Nutritional concepts (e.g., \"Are carbs bad?\")\n",
    "    5. Meal planning or preparation in general\n",
    "    6. General questions about meal timing\n",
    "    7. Impact of meals on health in general\n",
    " \n",
    "    Key Distinction: A meal question can be identified either by explicit food references OR by contextual clues indicating\n",
    "    the user is referring to a specific food item/image, even if not directly stated.\n",
    "\n",
    "2. HEALTH INTENT (Only for general health topics):\n",
    "   ONLY for general health questions NOT about specific foods:\n",
    "   - Questions about nutrition, health impacts, wellness\n",
    "   - General dietary advice\n",
    "   - Health benefits or risks\n",
    "   - Nutritional information requests\n",
    "   - General wellness principles\n",
    "   - Disease prevention/management\n",
    "   - Dietary patterns (not specific meals)\n",
    "   - General nutritional guidelines\n",
    "\n",
    "3. PRODUCT RECOMMENDATION INTENT:\n",
    "   - Expanded Detection Criteria:\n",
    "\n",
    "     1. HEALTH PRODUCT-SPECIFIC TRIGGERS:\n",
    "        - Questions specifically about health/wellness products or supplements:\n",
    "          * \"What supplements should I take for...\"\n",
    "          * \"Which products are good for...\"\n",
    "          * \"Recommend something for...\"\n",
    "          * \"Best supplements/products for...\"\n",
    "        \n",
    "        - Health conditions requiring product solutions:\n",
    "          * Sleep Health -> sleep supplements\n",
    "          * Bone Health -> calcium supplements\n",
    "          * Brain Health -> cognitive supplements\n",
    "          * Joint Health -> joint support products\n",
    "          * Vision Health -> eye health products\n",
    "          * Heart Health -> heart supplements\n",
    "          * Digestive Health -> probiotics, digestive aids\n",
    "          * Immunity Health -> immune boosters\n",
    "          * Energy -> energy supplements\n",
    "          * Weight Management -> weight management products\n",
    "\n",
    "     2. PRODUCT-SPECIFIC LANGUAGE:\n",
    "        Must include explicit or implicit references to:\n",
    "          * Supplements\n",
    "          * Vitamins\n",
    "          * Nutritional products\n",
    "          * Health aids\n",
    "          * Wellness products\n",
    "          * Natural remedies\n",
    "          * Dietary supplements\n",
    "\n",
    "     3. EXCLUDE BUSINESS/SALES QUERIES:\n",
    "        Do NOT classify as product intent if question involves:\n",
    "          * Business opportunity\n",
    "          * Sales plans\n",
    "          * Commission structures\n",
    "          * Marketing strategies\n",
    "          * Distributor relationships\n",
    "          * Business metrics\n",
    "          * Sales techniques\n",
    "          * Revenue/income questions\n",
    "          * Partner programs\n",
    "          * Business training\n",
    "          * Leadership levels\n",
    "          * Recruitment\n",
    "          * Business meetings\n",
    "          * Sales targets\n",
    "          * Market expansion\n",
    "\n",
    "     4. INTENT VALIDATION:\n",
    "        Question must satisfy BOTH:\n",
    "          a) Focus on health/wellness products\n",
    "          b) Seek product recommendation or information\n",
    "        \n",
    "        Examples:\n",
    "        ✓ \"What supplements are good for joint pain?\"\n",
    "        ✓ \"Recommend products for better sleep\"\n",
    "        ✗ \"How does the Amway compensation plan work?\"\n",
    "        ✗ \"Tips for growing my business\"\n",
    "\n",
    "   Detection Strategy:  \n",
    "   - Look for explicit product needs or implied product solutions\n",
    "   - Verify health/wellness context\n",
    "   - Check against business/sales exclusion list\n",
    "   - Confirm consumer (not business) perspective\n",
    "\n",
    "4. INCORRECT QUESTION INTENT:\n",
    "   - Question is not about health, nutrition, wellness,related to diseases, meal plate, or product recommendation\n",
    "\n",
    "   Detection Strategy:  \n",
    "   - Analyze entire question context, not just explicit product mentions\n",
    "   - Use natural language processing to detect underlying product-seeking intent\n",
    "   - Consider both direct and indirect signals of product recommendation needs\n",
    "\n",
    "   Exclusion Criteria:\n",
    "   - Pure informational queries without solution-seeking language\n",
    "   - Academic or research-oriented questions\n",
    "   - Purely conceptual health discussions\n",
    "\n",
    "Consider:\n",
    "- Multiple intents can be true simultaneously\n",
    "- Image context significantly increases likelihood of meal plate intent\n",
    "- Be particularly sensitive to demonstrative pronouns with images\n",
    "\n",
    "Classify the intents and explain your reasoning.\"\"\",\n",
    "        input_variables=[\"question\", \"has_image\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "    result = chain.invoke({\"question\": question, \"has_image\": has_image})\n",
    "\n",
    "    intents = []\n",
    "    if result.health_intent:\n",
    "        intents.append(\"health\")\n",
    "    if result.meal_plate_intent:\n",
    "        intents.append(\"meal_plate\")\n",
    "    if result.product_intent:\n",
    "        intents.append(\"product\")\n",
    "    if result.incorrect_question:\n",
    "        intents.append(\"others\")\n",
    "    \n",
    "\n",
    "    return intents\n",
    "\n",
    "\n",
    "def decompose_complex_query(question: str, has_image: bool) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Intelligently decompose complex queries while avoiding unnecessary breakdown.\n",
    "\n",
    "    Args:\n",
    "        question (str): The original user question\n",
    "        has_image (bool): Whether an image is present\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of decomposed query dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    class QueryDecomposition(BaseModel):\n",
    "        \"\"\"Structured output for query decomposition.\"\"\"\n",
    "\n",
    "        should_decompose: bool = Field(\n",
    "            description=\"Determine if the query needs to be broken down\"\n",
    "        )\n",
    "        sub_queries: Optional[List[str]] = Field(\n",
    "            description=\"List of focused sub-queries if decomposition is needed\"\n",
    "        )\n",
    "        rationale: Optional[str] = Field(\n",
    "            description=\"Explanation for decomposition decision\"\n",
    "        )\n",
    "\n",
    "    # Configure LLM for structured output\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    llm_with_tool = model.with_structured_output(QueryDecomposition)\n",
    "\n",
    "    # Comprehensive decomposition prompt\n",
    "    decomposition_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert query analyzer. Determine if a query truly requires decomposition.\n",
    "Analyze the query carefully and provide ONLY THE MOST ESSENTIAL sub-queries.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. MINIMIZE the number of sub-queries - each additional query costs time and resources\n",
    "2. Only include sub-queries that provide UNIQUE, NECESSARY information\n",
    "3. Avoid redundant or overlapping questions\n",
    "4. If a single focused query can capture the essence, DO NOT add more\n",
    "\n",
    "EXAMPLES OF GOOD DECOMPOSITION:\n",
    "\n",
    "BAD:\n",
    "Question: \"What are herbal remedies for headaches?\"\n",
    "❌ Sub-queries:\n",
    "   - \"What herbal remedies help with headaches?\"\n",
    "   - \"How effective are herbal remedies for headaches?\"  [Redundant]\n",
    "   - \"What is the history of herbal headache treatments?\" [Unnecessary]\n",
    "\n",
    "GOOD:\n",
    "✅ Single query is sufficient: \"What are herbal remedies for headaches?\"\n",
    "\n",
    "BAD:\n",
    "Question: \"How do diet and exercise affect diabetes management?\"\n",
    "❌ Sub-queries:\n",
    "   - \"How does diet affect diabetes?\"\n",
    "   - \"How does exercise affect diabetes?\"\n",
    "   - \"What is the relationship between diet and exercise?\" [Unnecessary]\n",
    "   - \"What is diabetes management?\" [Too basic]\n",
    "\n",
    "GOOD:\n",
    "✅ Sub-queries:\n",
    "   - \"How does diet affect diabetes management?\"\n",
    "   - \"How does exercise affect diabetes management?\"\n",
    "\n",
    "Remember: Each additional sub-query MUST provide essential, non-redundant information.\n",
    "\n",
    "Decomposition Criteria:\n",
    "1. ONLY decompose if the query is GENUINELY COMPLEX\n",
    "2. Do NOT break down simple, straightforward questions\n",
    "3. Focus on queries that have multiple distinct aspects or require multifaceted investigation\n",
    "\n",
    "Guidelines for Decomposition:\n",
    "- Simple definition or single-concept questions: DO NOT DECOMPOSE\n",
    "- Questions with multiple independent aspects: DECOMPOSE\n",
    "- Queries requiring exploration of different dimensions: DECOMPOSE\n",
    "\n",
    "Examples:\n",
    "1. \"What is gut health?\" \n",
    "   - Should Decompose: NO\n",
    "   - Sub-query: None\n",
    "   - Rationale: Simple definition query\n",
    "\n",
    "2. \"How can I improve my diet to manage diabetes and reduce heart risk?\"\n",
    "   - Should Decompose: YES\n",
    "   - Potential Sub-queries:\n",
    "     * \"What dietary changes help manage diabetes?\"\n",
    "     * \"What nutritional strategies reduce heart disease risk?\"\n",
    "\n",
    "3. \"What are the symptoms, causes, and treatments for rheumatoid arthritis?\"\n",
    "   - Should Decompose: YES\n",
    "   - Potential Sub-queries:\n",
    "     * \"What are the primary symptoms of rheumatoid arthritis?\"\n",
    "     * \"What causes rheumatoid arthritis?\"\n",
    "     * \"What are the current treatment options for rheumatoid arthritis?\"\n",
    "\n",
    "Original Question: {question}\n",
    "Image Present: {has_image}\n",
    "\n",
    "Make a precise determination: Should this query be decomposed?\"\"\",\n",
    "        input_variables=[\"question\", \"has_image\"],\n",
    "    )\n",
    "\n",
    "    # Create processing chain\n",
    "    chain = decomposition_prompt | llm_with_tool\n",
    "\n",
    "    try:\n",
    "        # Analyze query complexity\n",
    "        decomposition_result = chain.invoke(\n",
    "            {\"question\": question, \"has_image\": has_image}\n",
    "        )\n",
    "\n",
    "        # If no decomposition needed, return original query\n",
    "        if not decomposition_result.should_decompose:\n",
    "            return [\n",
    "                {\n",
    "                    \"id\": \"original_query\",\n",
    "                    \"text\": question,\n",
    "                    \"original_context\": {\n",
    "                        \"full_question\": question,\n",
    "                        \"has_image\": has_image,\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        # If decomposition is suggested\n",
    "        sub_queries = decomposition_result.sub_queries or [question]\n",
    "\n",
    "        # Prepare detailed sub-queries with context\n",
    "        processed_sub_queries = []\n",
    "        for idx, sub_query in enumerate(sub_queries, 1):\n",
    "            processed_sub_queries.append(\n",
    "                {\n",
    "                    \"id\": f\"sub_query_{idx}\",\n",
    "                    \"text\": sub_query,\n",
    "                    \"original_context\": {\n",
    "                        \"full_question\": question,\n",
    "                        \"has_image\": has_image,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Log decomposition rationale\n",
    "        print(f\"Decomposition Rationale: {decomposition_result.rationale}\")\n",
    "\n",
    "        return processed_sub_queries\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback mechanism\n",
    "        print(f\"Query decomposition error: {e}\")\n",
    "        return [\n",
    "            {\n",
    "                \"id\": \"original_query\",\n",
    "                \"text\": question,\n",
    "                \"original_context\": {\"full_question\": question, \"has_image\": has_image},\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "def intent_agent(state: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Enhanced intent agent with query decomposition and intelligent routing.\n",
    "\n",
    "    Args:\n",
    "        state (Dict): Current conversation state\n",
    "\n",
    "    Returns:\n",
    "        Dict: Updated conversation state with decomposed queries and agent routing\n",
    "    \"\"\"\n",
    "    print(\"\\n---ADVANCED INTENT AGENT---\")\n",
    "    # print(\"a\")\n",
    "\n",
    "    # Extract core information\n",
    "    question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    has_image = state[\"user_input\"].image is not None\n",
    "\n",
    "    # Decompose query\n",
    "    decomposed_queries = decompose_complex_query(question, has_image)\n",
    "\n",
    "    # Prepare state for decomposed processing\n",
    "    state[\"decomposed_queries\"] = decomposed_queries\n",
    "    state[\"active_agents\"] = set()\n",
    "    state[\"agent_responses\"] = {}\n",
    "\n",
    "    # Process each sub-query for intent detection\n",
    "    for sub_query in decomposed_queries:\n",
    "        sub_query_text = sub_query[\"text\"]\n",
    "\n",
    "        # Detect intents for each sub-query\n",
    "        intents = detect_intent(sub_query_text, has_image)\n",
    "        print(f\"Sub-query: {sub_query_text}\")\n",
    "        print(f\"Detected intents: {intents}\")\n",
    "\n",
    "        # Initialize intent_queries dictionary if it doesn't exist\n",
    "        if \"intent_queries\" not in state:\n",
    "            state[\"intent_queries\"] = {\n",
    "                \"retrieve\": [],\n",
    "                \"meal_plate\": [],\n",
    "                \"product_recommendation\": [],\n",
    "                \"others\": [],\n",
    "            }\n",
    "\n",
    "        # Add the sub-query to each detected intent's list\n",
    "        for intent in intents:\n",
    "            if intent == \"health\":\n",
    "                state[\"intent_queries\"][\"retrieve\"].append(sub_query)\n",
    "            elif intent == \"meal_plate\":\n",
    "                state[\"intent_queries\"][\"meal_plate\"].append(sub_query)\n",
    "            elif intent == \"product\":\n",
    "                state[\"intent_queries\"][\"product_recommendation\"].append(sub_query)\n",
    "            elif intent == \"others\":\n",
    "                state[\"intent_queries\"][\"others\"].append(sub_query)\n",
    "    return state\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def process_parallel(state: Dict) -> Dict:\n",
    "    \"\"\"Process multiple agents in parallel using ThreadPoolExecutor.\"\"\"\n",
    "\n",
    "    # Initialize active agents based on non-empty intent queries\n",
    "    state[\"active_agents\"] = [\n",
    "        agent for agent, queries in state[\"intent_queries\"].items() if queries\n",
    "    ]\n",
    "\n",
    "    def run_agent(agent_name: str, query_text: str) -> Dict:\n",
    "        \"\"\"Execute a specific agent based on its name.\"\"\"\n",
    "\n",
    "        try:\n",
    "            if agent_name == \"retrieve\":\n",
    "                ans = retrieve(state, query_text)\n",
    "                return {\"retrieve\": ans}\n",
    "            elif agent_name == \"meal_plate\":\n",
    "                return {\"meal_plate\": analyze_meal_question(state, query_text)}\n",
    "            elif agent_name == \"product_recommendation\":\n",
    "                return {\"product_recommendation\": recommend_products(state, query_text)}\n",
    "            elif agent_name == \"others\":\n",
    "                return {\"others\": others(state, query_text)}\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {agent_name} agent: {e}\")\n",
    "            return {}\n",
    "\n",
    "    # Create a list of all agent-query pairs that need to be processed\n",
    "    agent_query_pairs = []\n",
    "    for agent, queries in state[\"intent_queries\"].items():\n",
    "        for query in queries:\n",
    "            agent_query_pairs.append((agent, query[\"text\"]))\n",
    "\n",
    "    print(\"agent_query_pairs\", agent_query_pairs)\n",
    "    # Use ThreadPoolExecutor to process all queries in parallel\n",
    "    with ThreadPoolExecutor(max_workers=len(agent_query_pairs)) as executor:\n",
    "        # Submit tasks for each agent-query pair\n",
    "        future_to_pair = {\n",
    "            executor.submit(run_agent, agent, query_text): (agent, query_text)\n",
    "            for agent, query_text in agent_query_pairs\n",
    "        }\n",
    "\n",
    "        # Initialize results storage\n",
    "        results = {\"retrieve\": [], \"meal_plate\": [], \"product_recommendation\": [], \"others\": []}\n",
    "\n",
    "        # Collect results\n",
    "        for future in as_completed(future_to_pair):\n",
    "            agent, _ = future_to_pair[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                # Append the result to the appropriate list\n",
    "                for agent_name, response in result.items():\n",
    "                    if response:  # Only append non-empty responses\n",
    "                        results[agent_name].append(response)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query for {agent}: {e}\")\n",
    "\n",
    "        # Merge results for each agent\n",
    "        merged_state = state.copy()\n",
    "        merged_state[\"agent_responses\"] = {}\n",
    "\n",
    "        # Handle each agent type separately\n",
    "        if results[\"retrieve\"]:\n",
    "            # Combine retrieved docs from all retrieve responses\n",
    "            all_docs = []\n",
    "            for response in results[\"retrieve\"]:\n",
    "                if isinstance(response, dict) and \"retrieved_docs\" in response:\n",
    "                    all_docs.extend(response[\"retrieved_docs\"])\n",
    "            merged_state[\"agent_responses\"][\"retrieve\"] = {\"retrieved_docs\": all_docs}\n",
    "\n",
    "        if results[\"meal_plate\"]:\n",
    "            # Join meal plate responses if they're strings, otherwise keep the last response\n",
    "            meal_responses = []\n",
    "            for response in results[\"meal_plate\"]:\n",
    "                if isinstance(response, str):\n",
    "                    meal_responses.append(response)\n",
    "                elif isinstance(response, dict) and \"messages\" in response:\n",
    "                    meal_responses.append(response[\"messages\"][0].content)\n",
    "            merged_state[\"agent_responses\"][\"meal_plate\"] = (\n",
    "                \"\\n\".join(meal_responses) if meal_responses else \"\"\n",
    "            )\n",
    "\n",
    "        # if results[\"product_recommendation\"]:\n",
    "        #     # Combine product recommendations\n",
    "        #     all_products = []\n",
    "        #     for response in results[\"product_recommendation\"]:\n",
    "        #         if isinstance(response, dict) and \"retrieved_products\" in response:\n",
    "        #             all_products.extend(response[\"retrieved_products\"])\n",
    "        #     merged_state[\"agent_responses\"][\"product_recommendation\"] = {\n",
    "        #         \"retrieved_products\": all_products\n",
    "        #     }\n",
    "        \n",
    "        if results[\"product_recommendation\"]:            \n",
    "            # Process and structure product recommendations\n",
    "            unique_products = {}\n",
    "            consultation_insight = \"\"\n",
    "            \n",
    "            for response in results[\"product_recommendation\"]:\n",
    "                if isinstance(response, dict):\n",
    "                    # Deduplicate products\n",
    "                    if \"retrieved_products\" in response:\n",
    "                        for product in response[\"retrieved_products\"]:\n",
    "                            unique_products[product['product_name']] = product\n",
    "                    \n",
    "                    # Capture consultation insight\n",
    "                    if \"retrieved_result\" in response:\n",
    "                        consultation_insight = response[\"retrieved_result\"]\n",
    "            \n",
    "            # Convert unique products to list\n",
    "            all_products = list(unique_products.values())\n",
    "            \n",
    "            # Structure the merged state with clear differentiation\n",
    "            merged_state[\"agent_responses\"][\"product_recommendation\"] = {\n",
    "                \"retrieved_products\": all_products,\n",
    "                \"consultation_insight\": consultation_insight\n",
    "            }\n",
    "\n",
    "        if results[\"others\"]:\n",
    "            all_docs = []\n",
    "            for response in results[\"others\"]:\n",
    "                if isinstance(response, dict) and \"others\" in response:\n",
    "                    all_docs.extend(response[\"others\"])\n",
    "            merged_state[\"agent_responses\"][\"others\"] = {\"others\": all_docs}\n",
    "    \n",
    "    return merged_state\n",
    "\n",
    "def get_bigquery_schema(project_id, dataset_id, table_id):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # Get the full table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Retrieve the table\n",
    "    table = client.get_table(table_ref)\n",
    "    \n",
    "    schema_details = []\n",
    "    for field in table.schema:\n",
    "        schema_details.append({\n",
    "            'name': field.name,\n",
    "            'type': field.field_type,\n",
    "            'mode': field.mode,\n",
    "        })\n",
    "    \n",
    "    return schema_details \n",
    "\n",
    "def execute_bigquery_query(query: str) -> str:\n",
    "    \"\"\"Execute BigQuery query and return results as formatted string\"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        \n",
    "        # Convert results to DataFrame and then to string\n",
    "        df = results.to_dataframe()\n",
    "        if len(df) > 10:  # Limit large results\n",
    "            df = df.head(10)\n",
    "        return df.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {str(e)}\"\n",
    "\n",
    "def clean_sql_query(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SQL query by removing code block syntax, various SQL tags, backticks,\n",
    "    prefixes, and unnecessary whitespace while preserving the core SQL query.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw SQL query text that may contain code blocks, tags, and backticks\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned SQL query\n",
    "    \"\"\"\n",
    "    # Step 1: Remove code block syntax and any SQL-related tags\n",
    "    # This handles variations like ```sql, ```SQL, ```SQLQuery, etc.\n",
    "    block_pattern = r\"```(?:sql|SQL|SQLQuery|mysql|postgresql)?\\s*(.*?)\\s*```\"\n",
    "    text = re.sub(block_pattern, r\"\\1\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Step 2: Handle \"SQLQuery:\" prefix and similar variations\n",
    "    # This will match patterns like \"SQLQuery:\", \"SQL Query:\", \"MySQL:\", etc.\n",
    "    prefix_pattern = r\"^(?:SQL\\s*Query|SQLQuery|MySQL|PostgreSQL|SQL)\\s*:\\s*\"\n",
    "    text = re.sub(prefix_pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Extract the first SQL statement if there's random text after it\n",
    "    # Look for a complete SQL statement ending with semicolon\n",
    "    sql_statement_pattern = r\"(SELECT.*?;)\"\n",
    "    sql_match = re.search(sql_statement_pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if sql_match:\n",
    "        text = sql_match.group(1)\n",
    "\n",
    "    # Step 4: Remove backticks around identifiers\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Step 5: Normalize whitespace\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 6: Preserve newlines for main SQL keywords to maintain readability\n",
    "    keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'HAVING', 'ORDER BY',\n",
    "               'LIMIT', 'JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'INNER JOIN',\n",
    "               'OUTER JOIN', 'UNION', 'VALUES', 'INSERT', 'UPDATE', 'DELETE']\n",
    "\n",
    "    # Case-insensitive replacement for keywords\n",
    "    pattern = '|'.join(r'\\b{}\\b'.format(k) for k in keywords)\n",
    "    text = re.sub(f'({pattern})', r'\\n\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 7: Final cleanup\n",
    "    # Remove leading/trailing whitespace and extra newlines\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_bigquery_query2(schema: str, question: str, state: Dict = None) -> str:\n",
    "    \"\"\"Generate BigQuery SQL query from natural language question\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    # Get user_id/abo_id from state if available\n",
    "    abo_id = state.get(\"user_id\") if state else None\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a data analyst who converts natural language questions into BigQuery SQL queries.\n",
    "        \n",
    "        IMPORTANT CONTEXT:\n",
    "        - The table is located at: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        - Current ABO ID (if needed): {abo_id}\n",
    "        \n",
    "        SCHEMA:\n",
    "        {schema}\n",
    "        \n",
    "        QUESTION: {question}\n",
    "        \n",
    "        QUERY GUIDELINES:\n",
    "        1. Always use the full table path: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        2. If the question implies personal data or \"my\" information, use the ABO ID filter\n",
    "        3. For general queries, don't include ABO ID filter\n",
    "        4. Always include appropriate LIMIT clause for large result sets\n",
    "        5. Use clear column aliases for better readability\n",
    "        \n",
    "        Examples:\n",
    "        - \"Show my affiliate ID\"\n",
    "        ```sql\n",
    "        SELECT aff_id \n",
    "        FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        WHERE global_account_id = {abo_id}\n",
    "        LIMIT 1\n",
    "        ```\n",
    "        \n",
    "        - \"Count all users\"\n",
    "        ```sql\n",
    "        SELECT COUNT(DISTINCT global_account_id) as total_users\n",
    "        FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        ```\n",
    "        \n",
    "        Write only the SQL query, nothing else. Ensure it's a valid BigQuery SQL query.\n",
    "        \"\"\",\n",
    "        input_variables=[\"schema\", \"question\", \"abo_id\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    ans =  chain.invoke({\n",
    "        \"schema\": schema, \n",
    "        \"question\": question,\n",
    "        \"abo_id\": abo_id if abo_id else \"NULL\"\n",
    "    })\n",
    "    return clean_sql_query(ans)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "def generate_bigquery_query(schema: str, question: str, state: Dict = None) -> str:\n",
    "    \"\"\"Generate BigQuery SQL query from natural language question\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    # Get user_id/abo_id from state if available\n",
    "    abo_id = state.get(\"user_id\") if state else None\n",
    "    \n",
    "    # Define few-shot examples\n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"How does my monthly volume this year compare to last year (higher or lower)?\",\n",
    "            \"query\": \"\"\"SELECT \n",
    "    CASE \n",
    "        WHEN current_year_avg_total_downline_pv_normalized_to_10k > last_year_avg_total_downline_pv_normalized_to_10k \n",
    "        THEN 'higher' \n",
    "        ELSE 'lower'\n",
    "    END AS current_year_volume_vs_last_year\n",
    "FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "WHERE global_account_id = {abo_id}\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"How did my total downline pv from last year compare to two years ago?\",\n",
    "            \"query\": \"\"\"SELECT \n",
    "    CASE \n",
    "        WHEN last_year_avg_total_downline_pv_normalized_to_10k > two_years_ago_avg_total_downline_pv_normalized_to_10k \n",
    "        THEN 'higher' \n",
    "        ELSE 'lower'\n",
    "    END AS last_year_volume_vs_two_years_ago\n",
    "FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "WHERE global_account_id = {abo_id}\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create example prompt template\n",
    "    example_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"{input}\\nSQLQuery:\"),\n",
    "        (\"ai\", \"{query}\")\n",
    "    ])\n",
    "\n",
    "    # Create few-shot prompt template\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_prompt=example_prompt,\n",
    "        examples=examples,\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "    # Create the main prompt template with few-shot examples\n",
    "    final_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a data analyst who converts natural language questions into BigQuery SQL queries.\n",
    "        \n",
    "        IMPORTANT CONTEXT:\n",
    "        - The table is located at: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        - Current ABO ID (if needed): {abo_id}\n",
    "        \n",
    "        SCHEMA:\n",
    "        {schema}\n",
    "        \n",
    "        QUERY GUIDELINES:\n",
    "        1. Always use the full table path: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        2. If the question implies personal data or \"my\" information, use the ABO ID filter\n",
    "        3. For general queries, don't include ABO ID filter\n",
    "        4. Always include appropriate LIMIT clause for large result sets\n",
    "        5. Use clear column aliases for better readability\n",
    "        \n",
    "        Write only the SQL query, nothing else. Ensure it's a valid BigQuery SQL query.\"\"\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{question}\\nSQLQuery:\"),\n",
    "    ])\n",
    "    \n",
    "    # Generate the query using the enhanced prompt\n",
    "    chain = final_prompt | model | StrOutputParser()\n",
    "    \n",
    "    query = chain.invoke({\n",
    "        \"schema\": schema,\n",
    "        \"question\": question,\n",
    "        \"abo_id\": abo_id if abo_id else \"NULL\",\n",
    "        \"input\": question  # Required for few-shot template\n",
    "    })\n",
    "    \n",
    "    return clean_sql_query(query)\n",
    "\n",
    "def others(state: Dict, query_text: str) -> Dict:\n",
    "    \"\"\"Handle general queries including database questions\"\"\"\n",
    "    print(\"\\n-----Running query analysis-----\")\n",
    "    project_id = \"amw-dna-coe-working-ds-dev\"\n",
    "    dataset_id = \"data_science\"\n",
    "    table_id = \"abo_info\" \n",
    "\n",
    "    # First determine if this is a database query\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    analyze_prompt = PromptTemplate(\n",
    "    template=\"\"\"Determine if this question requires database lookup or should be answered from the sales plan knowledge base.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    CLASSIFICATION RULES:\n",
    "\n",
    "    1. DATABASE QUERIES (Return 'database'):\n",
    "       Any questions about:\n",
    "       a) Personal Information:\n",
    "          - ABO/Affiliate IDs\n",
    "          - Account details\n",
    "          - Registration dates\n",
    "          - Personal status/level\n",
    "          - Individual qualifications\n",
    "       \n",
    "       b) Numerical/Statistical Data:\n",
    "          - Member counts\n",
    "          - Performance metrics\n",
    "          - Sales figures\n",
    "          - Achievement statistics\n",
    "       \n",
    "       c) Relationship Data:\n",
    "          - Upline/downline information\n",
    "          - Team structure\n",
    "          - Sponsor details\n",
    "       \n",
    "       d) Transaction/History:\n",
    "          - Purchase history\n",
    "          - Commission records\n",
    "          - Point calculations\n",
    "          - PV/BV queries\n",
    "\n",
    "    2. SALES PLAN QUERIES (Return 'other'):\n",
    "       Any questions about:\n",
    "       a) Business Understanding:\n",
    "          - How the compensation plan works\n",
    "          - Commission structures\n",
    "          - Bonus calculations\n",
    "          - Leadership levels\n",
    "          - Qualification requirements\n",
    "       \n",
    "       b) Program Information:\n",
    "          - Business opportunity\n",
    "          - Sales plan details\n",
    "          - Reward programs\n",
    "          - Recognition systems\n",
    "       \n",
    "       c) General Knowledge:\n",
    "          - Business policies\n",
    "          - Company procedures\n",
    "          - Program benefits\n",
    "          - Growth opportunities\n",
    "\n",
    "    Examples:\n",
    "    DATABASE:\n",
    "    - \"What's my ABO ID?\" -> 'database'\n",
    "    - \"Show my current PV\" -> 'database'\n",
    "    - \"Who is in my downline?\" -> 'database'\n",
    "    - \"Check my qualification status\" -> 'database'\n",
    "\n",
    "    SALES PLAN (OTHER):\n",
    "    - \"How do I reach Platinum level?\" -> 'other'\n",
    "    - \"Explain the compensation structure\" -> 'other'\n",
    "    - \"What are the leadership qualifications?\" -> 'other'\n",
    "    - \"Tell me about the bonus program\" -> 'other'\n",
    "\n",
    "    Return ONLY 'database' for data lookup queries,\n",
    "    return 'other' for sales plan and general information questions.\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "    \n",
    "    analyze_chain = analyze_prompt | model | StrOutputParser()\n",
    "    \n",
    "    query_type = analyze_chain.invoke({\"question\": query_text}).strip().lower()\n",
    "    print(f\"Detected query type: {query_type}\")\n",
    "\n",
    "    if query_type == 'database':\n",
    "        try:\n",
    "            # Get schema\n",
    "            schema = get_bigquery_schema(project_id, dataset_id, table_id)\n",
    "            \n",
    "            # Generate SQL query\n",
    "            sql_query = generate_bigquery_query(schema, query_text, state)\n",
    "            print(f\"------Generated SQL query:--------/n {sql_query}\")\n",
    "            \n",
    "            # Execute query\n",
    "            query_results = execute_bigquery_query(sql_query)\n",
    "            \n",
    "            # Generate natural language response\n",
    "            response_prompt = PromptTemplate(\n",
    "                template=\"\"\"Based on the following database query and results, provide a natural language response.\n",
    "                \n",
    "                Question: {question}\n",
    "                SQL Query: {sql_query}\n",
    "                Results: {results}\n",
    "                \n",
    "                Provide a clear, concise explanation of the results in natural language.\n",
    "                \"\"\",\n",
    "                input_variables=[\"question\", \"sql_query\", \"results\"]\n",
    "            )\n",
    "            \n",
    "            response_chain = response_prompt | model | StrOutputParser()\n",
    "            natural_response = response_chain.invoke({\n",
    "                \"question\": query_text,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"results\": query_results\n",
    "            })\n",
    "            \n",
    "            return {\"others\": [{\"content\": natural_response, \"source\": \"BigQuery Database\"}]}\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing database query: {str(e)}\"\n",
    "            return {\"others\": [{\"content\": error_msg, \"source\": \"Error\"}]}\n",
    "    else:\n",
    "        print(\"-----Processing sales plan query-----\")\n",
    "        # Initialize Discovery Engine client and conversation\n",
    "        project_id = \"amw-dna-coe-working-ds-dev\"\n",
    "        data_stores = [\n",
    "            {\"location\": \"global\", \"data_store_id\": \"abo-sales-plan-hw_1734077229422\"},\n",
    "        ]\n",
    "        user_id = state[\"user_id\"]\n",
    "        manager = chat_manager_handler.get_current_manager(user_id)\n",
    "        \n",
    "        # Create Discovery Engine client\n",
    "        client_options = ClientOptions(api_endpoint=\"global-discoveryengine.googleapis.com\")\n",
    "        client = discoveryengine.ConversationalSearchServiceClient(client_options=client_options)\n",
    "        \n",
    "        # Create conversation\n",
    "        conversation = client.create_conversation(\n",
    "            parent=client.data_store_path(\n",
    "                project=project_id,\n",
    "                location=\"global\",\n",
    "                data_store=data_stores[0][\"data_store_id\"],\n",
    "            ),\n",
    "            conversation=discoveryengine.Conversation(),\n",
    "        )\n",
    "\n",
    "        # Make the search request\n",
    "        request = discoveryengine.ConverseConversationRequest(\n",
    "            name=conversation.name,\n",
    "            query=discoveryengine.TextInput(input=query_text),\n",
    "            serving_config=client.serving_config_path(\n",
    "                project=project_id,\n",
    "                location=\"global\",\n",
    "                data_store=data_stores[0][\"data_store_id\"],\n",
    "                serving_config=\"default_config\",\n",
    "            ),\n",
    "            summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
    "                include_citations=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.converse_conversation(request)\n",
    "            results = []\n",
    "\n",
    "            for result in response.search_results:\n",
    "                result_data = result.document.derived_struct_data\n",
    "                content = result_data.get(\"snippets\", [{}])[0].get(\"snippet\", \"\")\n",
    "                source = result_data.get(\"link\", \"Unknown source\")\n",
    "                title = result_data.get(\"title\", \"Untitled\")\n",
    "                results.append({\n",
    "                    \"content\": content,\n",
    "                    \"source\": source,\n",
    "                    \"title\": title,\n",
    "                })\n",
    "\n",
    "            ranked_results = rank_documents(manager, results, query_text)\n",
    "            # print(\"ranked_results\",ranked_results)\n",
    "            return {\"others\": ranked_results}\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing sales plan query: {str(e)}\"\n",
    "            return {\"others\": [{\"content\": error_msg, \"source\": \"Error\"}]}\n",
    "        \n",
    "\n",
    "def recommend_products2(state: Dict, query_text: str) -> Dict:\n",
    "    print(\"running recommend_products\")\n",
    "    project_id = \"amw-dna-coe-working-ds-dev\"\n",
    "    location = \"global\"\n",
    "    engine_id = \"product-search-app_1732269184635\"\n",
    "    # search_query = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    search_query = query_text\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search app serving config\n",
    "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
    "\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "        page_size=10,\n",
    "        # content_search_spec=content_search_spec,\n",
    "        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n",
    "            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n",
    "        ),\n",
    "        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
    "            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = client.search(request)\n",
    "    result = next(response.pages)\n",
    "\n",
    "    search_results = result.results\n",
    "    all_search_result = []\n",
    "\n",
    "    for i, result1 in enumerate(search_results, 1):\n",
    "        result_doc = result1.document.struct_data\n",
    "        search_result = {}\n",
    "        search_result[\"product_name\"] = result_doc[\"product_name\"]\n",
    "        search_result[\"product_url\"] = result_doc[\"product_url\"]\n",
    "\n",
    "        all_search_result.append(search_result)\n",
    "    print(\"all_search_result\", all_search_result)\n",
    "    top_n = 3\n",
    "    return {\n",
    "        \"retrieved_products\": all_search_result[:top_n],\n",
    "    }\n",
    "\n",
    "def recommend_products(state: Dict, query_text: str) -> Dict:\n",
    "    # print(\"running recommend_products\")\n",
    "    project_id = \"amw-dna-coe-working-ds-dev\"\n",
    "    location = \"global\"\n",
    "    engine_id = \"product-search-app_1732269184635\"\n",
    "    # search_query = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    search_query = query_text\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.SearchServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the search app serving config\n",
    "    serving_config = f\"projects/{project_id}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/default_config\"\n",
    "\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "        page_size=10,\n",
    "        # content_search_spec=content_search_spec,\n",
    "        query_expansion_spec=discoveryengine.SearchRequest.QueryExpansionSpec(\n",
    "            condition=discoveryengine.SearchRequest.QueryExpansionSpec.Condition.AUTO,\n",
    "        ),\n",
    "        spell_correction_spec=discoveryengine.SearchRequest.SpellCorrectionSpec(\n",
    "            mode=discoveryengine.SearchRequest.SpellCorrectionSpec.Mode.AUTO\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = client.search(request)\n",
    "    result = next(response.pages)\n",
    "\n",
    "    search_results = result.results\n",
    "    all_search_result = []\n",
    "    \n",
    "    product_search_result_global = []\n",
    "    manager.products = \"\"\n",
    "    # print(\"Results from product: \", search_results)\n",
    "    for i, result1 in enumerate(search_results, 1):\n",
    "        result_doc = result1.document.struct_data\n",
    "        search_result = {}\n",
    "        products_global_data = {}\n",
    "        search_result[\"product_name\"] = result_doc[\"product_name\"]\n",
    "        search_result[\"product_url\"] = result_doc[\"product_url\"]\n",
    "        \n",
    "        products_global_data[\"product_name\"] = result_doc[\"product_name\"]\n",
    "        products_global_data[\"product_description\"] = result_doc[\"product_description\"]\n",
    "        \n",
    "        product_search_result_global.append(products_global_data)\n",
    "        all_search_result.append(search_result)\n",
    "        \n",
    "    top_n = 3    \n",
    "    product_search_result_global = product_search_result_global[:top_n]    \n",
    "    manager.products += str(product_search_result_global)\n",
    "    \n",
    "    shopping_result = shopping(state, query_text)\n",
    "    \n",
    "    print(\"Result from shopping Intent to Product one: \", shopping_result)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"retrieved_products\": all_search_result[:top_n],\n",
    "        \"retrieved_result\": shopping_result\n",
    "    }\n",
    "\n",
    "def compute_similarity(query_embedding, doc_embedding):\n",
    "    \"\"\"Compute cosine similarity between embeddings.\"\"\"\n",
    "    return np.dot(query_embedding, doc_embedding) / (\n",
    "        np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "    )\n",
    "\n",
    "\n",
    "def rank_documents(\n",
    "    manager: EnhancedConversationManager, docs: List[Dict], query: str\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Rank documents based on relevance to query.\"\"\"\n",
    "    query_embedding = manager.model.encode([query])[0]\n",
    "\n",
    "    doc_similarities = []\n",
    "    for doc in docs:\n",
    "        content = doc[\"content\"]\n",
    "        doc_embedding = manager.model.encode([content])[0]\n",
    "        similarity = compute_similarity(query_embedding, doc_embedding)\n",
    "        doc_similarities.append((similarity, doc))\n",
    "\n",
    "    threshold = 0.3\n",
    "    ranked_docs = [\n",
    "        doc\n",
    "        for score, doc in sorted(doc_similarities, key=lambda x: x[0], reverse=True)\n",
    "        if score > threshold\n",
    "    ]\n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "def search_data_store(\n",
    "    manager: EnhancedConversationManager, query: str, store_info: Dict[str, str]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Search a single data store and return results.\"\"\"\n",
    "    client = manager.clients[store_info[\"data_store_id\"]]\n",
    "    conversation = manager.conversations[store_info[\"data_store_id\"]]\n",
    "    request = discoveryengine.ConverseConversationRequest(\n",
    "        name=conversation.name,\n",
    "        query=discoveryengine.TextInput(input=query),\n",
    "        serving_config=client.serving_config_path(\n",
    "            project=manager.project_id,\n",
    "            location=store_info[\"location\"],\n",
    "            data_store=store_info[\"data_store_id\"],\n",
    "            serving_config=\"default_config\",\n",
    "        ),\n",
    "        summary_spec=discoveryengine.SearchRequest.ContentSearchSpec.SummarySpec(\n",
    "            include_citations=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    response = client.converse_conversation(request)\n",
    "    results = []\n",
    "\n",
    "    for result in response.search_results:\n",
    "        result_data = result.document.derived_struct_data\n",
    "        content = result_data.get(\"snippets\", [{}])[0].get(\"snippet\", \"\")\n",
    "        source = result_data.get(\"link\", \"Unknown source\")\n",
    "        title = result_data.get(\"title\", \"Untitled\")\n",
    "        results.append(\n",
    "            {\n",
    "                \"content\": content,\n",
    "                \"source\": source,\n",
    "                \"title\": title,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def search_and_rank(manager: EnhancedConversationManager, question: str) -> Dict:\n",
    "    \"\"\"Process a question and return a comprehensive response.\"\"\"\n",
    "    # Search all data stores in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        all_results = []\n",
    "        futures = []\n",
    "\n",
    "        # Create futures with correct arguments\n",
    "        for store_info in manager.data_stores:\n",
    "            future = executor.submit(\n",
    "                search_data_store,\n",
    "                manager,  # Pass the manager instance\n",
    "                question,  # Pass the question\n",
    "                store_info,  # Pass the store info dictionary\n",
    "            )\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            try:\n",
    "                results = future.result()\n",
    "                all_results.extend(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching data store: {e}\")\n",
    "\n",
    "    # Rank and filter results\n",
    "    ranked_results = rank_documents(\n",
    "        manager, all_results, question\n",
    "    )  # Pass manager and query\n",
    "    # print(\"ranked docs\",ranked_results)\n",
    "    return ranked_results\n",
    "\n",
    "\n",
    "def retrieve(state: Dict, query_text: str) -> Dict:\n",
    "    print(\"\\n---RETRIEVE DOCUMENTS---\")\n",
    "    # Get the corrected question from state\n",
    "    # corrected_question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    corrected_question = query_text\n",
    "    print(f\"Processing question: {corrected_question}\")\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "\n",
    "    # Pass the manager instance and corrected question\n",
    "    manager.current_context = search_and_rank(manager, corrected_question)\n",
    "    top_n = 8\n",
    "    # print(\"manager.current_context\",manager.current_context[:top_n])\n",
    "\n",
    "    state[\"retrieved_docs\"] = manager.current_context[:top_n]\n",
    "    return {\n",
    "        \"retrieved_docs\": manager.current_context[:top_n],\n",
    "        \"corrected_question\": corrected_question,  # Maintain the corrected question\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def parse_response(response):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "       dict: Dictionary containing parsed sections with preserved formatting\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary with empty default values\n",
    "    result = {\"answer\": \"\", \"sources\": [], \"more_topics\": []}\n",
    "\n",
    "    # Split the response into lines\n",
    "    lines = response.split(\"\\n\")\n",
    "\n",
    "    current_section = \"answer\"\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Check for section transitions\n",
    "        if stripped_line.lower().startswith(\"sources:\"):\n",
    "            current_section = \"sources\"\n",
    "            continue\n",
    "        elif stripped_line.lower().startswith(\"would you like to know more about:\"):\n",
    "            current_section = \"more_topics\"\n",
    "            continue\n",
    "\n",
    "        # Process line based on current section\n",
    "        if current_section == \"answer\":\n",
    "            # Preserve original line formatting for answer\n",
    "            if result[\"answer\"]:\n",
    "                result[\"answer\"] += \"\\n\" + line\n",
    "            else:\n",
    "                result[\"answer\"] = line\n",
    "        elif current_section == \"sources\":\n",
    "            # Handle sources with bullet points or dashes\n",
    "            if stripped_line.startswith(\"-\") or stripped_line.startswith(\"•\"):\n",
    "                result[\"sources\"].append(stripped_line[1:].strip())\n",
    "            elif stripped_line:\n",
    "                result[\"sources\"].append(stripped_line)\n",
    "        elif current_section == \"more_topics\":\n",
    "            # Handle more topics with bullet points or dashes\n",
    "            if stripped_line.startswith(\"-\") or stripped_line.startswith(\"•\"):\n",
    "                result[\"more_topics\"].append(stripped_line[1:].strip())\n",
    "            elif stripped_line and not stripped_line.lower().startswith(\n",
    "                \"would you like to know more about\"\n",
    "            ):\n",
    "                result[\"more_topics\"].append(stripped_line)\n",
    "\n",
    "    # Clean up answer by removing leading/trailing whitespace while preserving internal formatting\n",
    "    result[\"answer\"] = result[\"answer\"].strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate(state: Dict) -> Dict:\n",
    "    \"\"\"Generate final response combining all agent outputs with proper formatting.\"\"\"\n",
    "    print(\"\\n---GENERATE ANSWER---\")\n",
    "    corrected_question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "    response = \"\"\n",
    "    # Prepare for relevance grading\n",
    "    class grade(BaseModel):\n",
    "        binary_score: Literal[\"yes\", \"no\"]\n",
    "\n",
    "    # Set up LLM for grading\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Gather health documents\n",
    "    health_docs = state.get(\"agent_responses\", {}).get(\"retrieve\", \"\")\n",
    "    health_context = \"\"\n",
    "    others_context = \"\" \n",
    "\n",
    "    consultation_insight = (\n",
    "        state.get(\"agent_responses\", {})\n",
    "        .get(\"product_recommendation\", {})\n",
    "        .get(\"consultation_insight\", \"\")\n",
    "    )\n",
    "\n",
    "    # Check if health docs are not empty and perform relevance grading\n",
    "    if (\n",
    "        health_docs\n",
    "        and isinstance(health_docs, dict)\n",
    "        and health_docs.get(\"retrieved_docs\")\n",
    "    ):\n",
    "        # Combine documents into context\n",
    "        docs_context = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"Document: {doc['content']}\\nSource: {doc['source']}\"\n",
    "                for doc in health_docs[\"retrieved_docs\"]\n",
    "            ]\n",
    "        )\n",
    "        health_context = docs_context\n",
    "\n",
    "    # Get meal response\n",
    "    meal_response = state.get(\"agent_responses\", {}).get(\"meal_plate\", \"\")\n",
    "\n",
    "    others_response = state.get(\"agent_responses\", {}).get(\"others\", \"\")\n",
    "    # print(\"others_response\",others_response)\n",
    "    if others_response and isinstance(others_response, dict) and others_response.get(\"others\") and others_response[\"others\"] != []:\n",
    "        others_context = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"Document: {doc['content']}\\nSource: {doc['source']}\"\n",
    "                for doc in others_response[\"others\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # print(\"others_context\",others_context)# Extract product recommendations\n",
    "    \n",
    "    product_recommendations = (\n",
    "        state.get(\"agent_responses\", {})\n",
    "        .get(\"product_recommendation\", {})\n",
    "        .get(\"retrieved_products\", [])\n",
    "    )\n",
    "\n",
    "    # Only proceed if at least one context is non-empty\n",
    "    full_response = \"\"\n",
    "    if health_context or meal_response or others_context or consultation_insight:\n",
    "        merge_prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a friendly and knowledgeable chatbot having a conversation with the user. Synthesize information from multiple sources into a coherent, well-structured response.\n",
    "\n",
    "**AVAILABLE INFORMATION:**\n",
    "Health Information:\n",
    "{health_context}        \n",
    "\n",
    "Meal Analysis:\n",
    "{meal_response}\n",
    "\n",
    "Product Information:\n",
    "{consultation_insight}\n",
    "\n",
    "Others:\n",
    "{others_context}\n",
    "\n",
    "**RESPONSE GUIDELINES:**\n",
    " \n",
    "1. **Conversational Style:**\n",
    "   - Use a friendly, casual tone while maintaining professionalism\n",
    "   - Keep responses brief and to-the-point (2-3 short paragraphs max)\n",
    "   - Break complex information into digestible chunks\n",
    "   - Use simple, everyday language\n",
    "   - Add brief conversational transitions when needed\n",
    " \n",
    "2. **Health & Safety Rules:**\n",
    " \n",
    "    FIRST: Check if question contains ANY of:\n",
    "    - Medical conditions (diabetes, arthritis, cancer, etc.)\n",
    "    - Diet/nutrition topics\n",
    "    - Treatments/medications\n",
    "    - Symptoms or health concerns\n",
    "    - Emergency situations\n",
    " \n",
    "    THEN:\n",
    "    - Use valid source URLs in plain text\n",
    "    - Include ALL applicable disclaimers from below:\n",
    " \n",
    "    **Required Disclaimers:**\n",
    " \n",
    "    FOR ANY DIET/NUTRITION CONTENT:\n",
    "    *Please note: For personalized dietary advice and meal planning tailored to your health needs, consult a registered dietitian.*\n",
    " \n",
    "    FOR ANY MEDICAL CONDITIONS:\n",
    "    *Important: This information is general education only. For medical advice specific to your [condition_name], please consult your healthcare provider.*\n",
    " \n",
    "    FOR ANY TREATMENTS/MEDICATIONS:\n",
    "    *Medical Disclaimer: Never start or modify any medication/treatment without consulting a qualified healthcare professional.*\n",
    " \n",
    "    FOR EMERGENCY TOPICS:\n",
    "    *EMERGENCY: If you're experiencing a medical emergency, contact emergency services immediately.*\n",
    " \n",
    "    **Disclaimer Rules:**\n",
    "    - Format in italics using asterisks\n",
    "    - Include BOTH medical and dietary disclaimers for condition-specific diet questions\n",
    "    - Always include condition name in medical disclaimer\n",
    "    - NOTE: Never skip disclaimers for health-related content\n",
    "\n",
    "3. **Answer Structure:**\n",
    "   - Start with a direct, concise answer\n",
    "   - Use bullet points for multiple items\n",
    "   - Keep explanations brief but clear\n",
    "   - Include source attribution naturally in conversation\n",
    " \n",
    "4. **Sources and Follow-ups:**\n",
    "   - End with \"Sources:\" followed by cited sources in plain text and complete URLs\n",
    "   - Add \"Would you like to know more about:\" followed by 2 brief, relevant follow-up topics (not questions)\n",
    "   - Keep both sections short and professional\n",
    " \n",
    "5. **Formatting:**\n",
    "   - Use simple markdown for readability\n",
    "   - Include paragraph breaks for easy reading\n",
    "   - Use bullet points sparingly and only when listing items\n",
    " \n",
    "6. **Source Compliance:**\n",
    "   - Stick to information from provided context\n",
    "   - Don't create additional information\n",
    "   - Attribute sources conversationally\n",
    "\n",
    "7. **Handling Irrelevant Questions:**\n",
    "- If others_response indicates irrelevant questions, acknowledge them professionally\n",
    "- Explain briefly why they're outside the scope of health/wellness\n",
    "- Suggest relevant alternatives when possible\n",
    "- Keep this section brief and at the end of the main answer\n",
    "- Use a friendly, helpful tone even when redirecting\n",
    " \n",
    "  \n",
    "Your response should end with:\n",
    " \n",
    "Sources:\n",
    "[List sources here in plain text]\n",
    " \n",
    "Would you like to know more about:\n",
    "• [Topic 1]\n",
    "• [Topic 2]\n",
    "\n",
    "Question: {corrected_question}\n",
    "\n",
    "Remember to be friendly and conversational while providing accurate, source-based information. Keep responses concise and engaging. Ensure smooth transitions between different types of information.\"\"\",\n",
    "            input_variables=[\"health_context\", \"meal_response\",\"others_context\", \"consultation_insight\",\"corrected_question\"],\n",
    "        )\n",
    "\n",
    "        generate_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "        chain = merge_prompt | generate_model | StrOutputParser()\n",
    "\n",
    "        # Generate the main response\n",
    "        response = chain.invoke(\n",
    "            {\n",
    "                \"health_context\": health_context,\n",
    "                \"meal_response\": meal_response,\n",
    "                \"others_context\": others_context,\n",
    "                \"consultation_insight\": consultation_insight,\n",
    "                \"corrected_question\": corrected_question,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    # Format product recommendations in markdown\n",
    "    if product_recommendations:\n",
    "        product_section = \"\\n\\n---\\n\\n**Recommended Products:**\\n\"\n",
    "        for product in product_recommendations:\n",
    "            product_section += (\n",
    "                f\"- [{product['product_name']}]({product['product_url']})\\n\"\n",
    "            )\n",
    "\n",
    "        # Append product recommendations to the response\n",
    "        \n",
    "        full_response = response + product_section\n",
    "    else:\n",
    "        full_response = response\n",
    "    \n",
    "    print(full_response)\n",
    "    \n",
    "\n",
    "    # Parse and store response\n",
    "    if response!=\"\":\n",
    "        result = parse_response(response)\n",
    "    else:\n",
    "        result = {\"answer\": \"\", \"sources\": [], \"more_topics\": []} \n",
    "    \n",
    "    translated_result = {}\n",
    "    if manager.language == \"en\":\n",
    "        pass\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            # Handle single string translation for 'answer'\n",
    "            future_to_key = {\n",
    "                executor.submit(\n",
    "                    translate_text, state, result[\"answer\"], manager.language\n",
    "                ): \"answer\"\n",
    "            }\n",
    "\n",
    "            # Handle list translation for 'more_topics'\n",
    "            if isinstance(result[\"more_topics\"], list):\n",
    "                # Translate each topic in the list\n",
    "                more_topics_futures = [\n",
    "                    executor.submit(translate_text, state, topic, manager.language)\n",
    "                    for topic in result[\"more_topics\"]\n",
    "                ]\n",
    "                future_to_key.update(\n",
    "                    {\n",
    "                        future: f\"more_topics_{i}\"\n",
    "                        for i, future in enumerate(more_topics_futures)\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                # If it's a single string, translate it like 'answer'\n",
    "                future_to_key[\n",
    "                    executor.submit(\n",
    "                        translate_text, result[\"more_topics\"], manager.language\n",
    "                    )\n",
    "                ] = \"more_topics\"\n",
    "            result[\"more_topics\"] = []\n",
    "            for future in as_completed(future_to_key):\n",
    "                key = future_to_key[future]\n",
    "                try:\n",
    "                    if key == \"answer\":\n",
    "                        result[\"answer\"] = future.result()\n",
    "                    elif key == \"more_topics\":\n",
    "                        result[\"more_topics\"] = future.result()\n",
    "                    elif key.startswith(\"more_topics_\"):\n",
    "                        # Reconstruct the list of translated topics\n",
    "                        result[\"more_topics\"].append(future.result())\n",
    "                except Exception as exc:\n",
    "                    print(f\"Translation for {key} generated an exception: {exc}\")\n",
    "                    if key == \"answer\":\n",
    "                        result[\"answer\"] = result[\"answer\"]\n",
    "                    elif key == \"more_topics\":\n",
    "                        result[\"more_topics\"] = result[\"more_topics\"]\n",
    "                    elif key.startswith(\"more_topics_\"):\n",
    "                        if \"more_topics\" not in translated_result:\n",
    "                            result[\"more_topics\"] = []\n",
    "                        translated_result[\"more_topics\"].append(\n",
    "                            result[\"more_topics\"][int(key.split(\"_\")[-1])]\n",
    "                        )\n",
    "    \n",
    "    if product_recommendations:\n",
    "        result[\"products\"] = product_section\n",
    "    manager.answer_history.append(result[\"answer\"])\n",
    "    manager.conversation_history.append(result)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=str(result))],\n",
    "        \"corrected_question\": corrected_question, \n",
    "    }\n",
    "\n",
    "def shopping(state: Dict, query_text: str) -> Dict:\n",
    "    print(\"\\n---SHOPPING AGENT---\")\n",
    "    corrected_question = query_text\n",
    "    user_id = state[\"user_id\"]\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"user_input\", \"product_description\"],\n",
    "        template=\"\"\"PRODUCT CONSULTATION PROTOCOL\n",
    "\n",
    "        CORE DIRECTIVE:\n",
    "            - Analyze user query: {user_input}\n",
    "            - Reference available product data: {product_description}\n",
    "            - Provide a direct, precise answer\n",
    "            - Use only factual product information\n",
    "            - Avoid unnecessary details\n",
    "            - Answer exactly what is asked\n",
    "            - Be concise and to the point\n",
    "\n",
    "        RESPONSE GUIDELINES:\n",
    "            1. Understand the specific question\n",
    "            2. Extract relevant product information\n",
    "            3. Deliver a clear, straightforward response\n",
    "            4. If no direct answer is possible, state limitations clearly\n",
    "\n",
    "        CRITICAL CONSTRAINTS:\n",
    "            - Maximum brevity\n",
    "            - Absolute precision\n",
    "            - Direct addressing of the query\n",
    "            - No speculative or generalized information\"\"\",\n",
    "    )\n",
    "    # Set up the language model with a specified temperature and model type\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "    chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "\n",
    "    # Invoke the chain to generate a refined question\n",
    "    result = chain.invoke(\n",
    "        {\n",
    "            \"user_input\": corrected_question,\n",
    "            \"product_description\": manager.products,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    output_text = result[\"text\"]\n",
    "    # print(result)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "\n",
    "def incorrect_question(state):\n",
    "    \"\"\"\n",
    "    Handle case when the question is deemed incorrect or irrelevant.\n",
    "    \"\"\"\n",
    "    print(\"---INCORRECT QUESTION---\")\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"The question appears to be incorrect or irrelevant to the available information.\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_meal_question(state, question: str) -> Dict:\n",
    "    print(\"---ANALYZE MEAL QUESTION---\",question)\n",
    "\n",
    "    class QuestionAnalysis(BaseModel):\n",
    "        \"\"\"Analysis result for the user's question.\"\"\"\n",
    "\n",
    "        is_meal_related: bool = Field(\n",
    "            description=\"Whether the question is related to meal analysis\"\n",
    "        )\n",
    "        api_call_type: str = Field(\n",
    "            description=\"Type of API call needed: 'image_scan', 'text_query', 'barcode_scan', or 'none'\"\n",
    "        )\n",
    "        reasoning: str = Field(description=\"Explanation for the decision\")\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "    llm_with_tool = model.with_structured_output(QuestionAnalysis)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Analyze the following user question and context:\n",
    "\n",
    "        User Question: {question}\n",
    "        Has Image Attachment: {has_image}\n",
    "        Image Path (if any): {image_path}\n",
    "\n",
    "        Determine if the question is related to meal plate analysis and what type of API call is needed.\n",
    "        The question is meal-related if it asks about food items, nutritional information, or involves analyzing a meal image or barcode.\n",
    "\n",
    "        Possible API call types:\n",
    "        1. 'image_scan': If there's an image attachment and the question involves analyzing the food/meal in the image\n",
    "        2. 'text_query': If the question is about specific food items or nutritional information without image analysis\n",
    "        3. 'barcode_scan': If there's an image attachment and the question involves scanning a barcode\n",
    "        4. 'none': If the question is not related to meal analysis\n",
    "\n",
    "        Consider:\n",
    "        - If there's an image attachment, prioritize image analysis unless explicitly asking for barcode scanning\n",
    "        - Text queries should only be used when no image is provided\n",
    "        - The presence of words like \"this\", \"in the image\", \"this meal\" with an image strongly suggests image scanning\n",
    "\n",
    "        Return your analysis with clear reasoning for the decision.\n",
    "        \"\"\",\n",
    "        input_variables=[\"question\", \"has_image\", \"image_path\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    # question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    user_input = state[\"user_input\"]\n",
    "    image_path = user_input.image.filename if user_input.image else None\n",
    "    has_image = user_input.image is not None\n",
    "\n",
    "    # Perform analysis\n",
    "    analysis_result = chain.invoke(\n",
    "        {\"question\": question, \"has_image\": has_image, \"image_path\": image_path}\n",
    "    )\n",
    "    print(\"-----analysis_result-----\",analysis_result) \n",
    "    # If not meal-related, return early\n",
    "    if not analysis_result.is_meal_related:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=\"Question is not related to meal analysis.\")\n",
    "            ],\n",
    "            \"question\": question,\n",
    "        }\n",
    "\n",
    "    # Perform API call based on analysis\n",
    "    try:\n",
    "        # Get OAuth Token\n",
    "        TOKEN = get_token(\n",
    "            \"https://api-dv.amwayglobal.com/rest/oauth2/v1/token\",\n",
    "            \"3hmyXKbHlA0ZLJ1Zjtg4G1X0l4srn0jIolK7pzB4EqiqBb1M\",\n",
    "            \"9Trey6amtSaRifSzU1HM2UlirkSLojkBCa0xWA51nUkyFeoGFFfVKWEuGdV8pNbu\",\n",
    "        )\n",
    "        print(\"-----token-----\",TOKEN)\n",
    "        # Prepare common headers\n",
    "        headers_o = {\n",
    "            \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "            \"x-hw-program\": \"mg_testing\",\n",
    "            \"x-abold\": \"mg_abo\",\n",
    "            \"x-mealtime\": \"\",\n",
    "            \"x-genai-vendor\": \"openai\",\n",
    "            \"x-country-code\": \"mg_testing\",\n",
    "        }\n",
    "\n",
    "        # Perform API call based on analysis type\n",
    "        if analysis_result.api_call_type == \"image_scan\" and has_image:\n",
    "            print(\"-----image scan-----\")\n",
    "            with open(image_path, \"rb\") as img_file:\n",
    "                files = {\"meal_image\": img_file}\n",
    "                print(\"-----files-----\",files)\n",
    "                response = requests.post(\n",
    "                    \"https://api-qa.amwayglobal.com/v1/health-wellbeing/mealanalyzer/meal-scan\",\n",
    "                    headers=headers_o,\n",
    "                    files=files,\n",
    "                    data={},\n",
    "                )\n",
    "            print(\"-----response-----\",response.json()) \n",
    "        elif analysis_result.api_call_type == \"text_query\":\n",
    "            print(\"-----text query-----\")\n",
    "            data = {\"meal_description\": question}\n",
    "            response = requests.post(\n",
    "                \"https://api-dv.amwayglobal.com/v1/health-wellbeing/mealanalyzer/meal-scan\",\n",
    "                headers=headers_o,\n",
    "                files={},\n",
    "                data=data,\n",
    "            )\n",
    "            print(\"-----response-----\",response.json())\n",
    "        elif analysis_result.api_call_type == \"barcode_scan\" and has_image:\n",
    "            with open(image_path, \"rb\") as img_file:\n",
    "                upc_file = {\"image\": img_file}\n",
    "                response = requests.post(\n",
    "                    \"https://api-dv.amwayglobal.com/v1/health-wellbeing/mealanalyzer/upc\",\n",
    "                    files=upc_file,\n",
    "                    headers=headers_o,\n",
    "                    data={},\n",
    "                )\n",
    "        else:\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(content=\"No applicable meal analysis method found.\")\n",
    "                ],\n",
    "                \"question\": question,\n",
    "            }\n",
    "\n",
    "        # Evaluate API response\n",
    "        answer = evaluate_api_response(state, response.json(), question)\n",
    "        return {\"messages\": [HumanMessage(content=answer)]}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"messages\": [HumanMessage(content=f\"Error in meal analysis: {str(e)}\")],\n",
    "            \"question\": question,\n",
    "        }\n",
    "\n",
    "\n",
    "def meal_image_scan(state):\n",
    "    \"\"\"\n",
    "    Handle case when the question is deemed incorrect or irrelevant.\n",
    "    \"\"\"\n",
    "    print(\"---Scanning the image---\")\n",
    "    user_input = state[\"user_input\"]\n",
    "    user_question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    if user_input.image:\n",
    "        TOKEN = get_token(\n",
    "            \"https://api-dv.amwayglobal.com/rest/oauth2/v1/token\",\n",
    "            \"3hmyXKbHlA0ZLJ1Zjtg4G1X0l4srn0jIolK7pzB4EqiqBb1M\",\n",
    "            \"9Trey6amtSaRifSzU1HM2UlirkSLojkBCa0xWA51nUkyFeoGFFfVKWEuGdV8pNbu\",\n",
    "        )\n",
    "        headers_o = {\n",
    "            \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "            \"x-hw-program\": \"mg_testing\",\n",
    "            \"x-abold\": \"mg_abo\",\n",
    "            \"x-mealtime\": \"\",\n",
    "            \"x-genai-vendor\": \"openai\",\n",
    "            \"x-country-code\": \"mg_testing\",\n",
    "        }\n",
    "        with open(user_input.image.filename, \"rb\") as img_file:\n",
    "            files = {\"meal_image\": img_file}\n",
    "            response = requests.post(\n",
    "                \"https://api-dv.amwayglobal.com/v1/health-wellbeing/mealanalyzer/meal-scan\",\n",
    "                headers=headers_o,\n",
    "                files=files,\n",
    "                data={},\n",
    "            )\n",
    "\n",
    "            # print(\"the response:\",response.json())\n",
    "            answer = evaluate_api_response(state, response.json(), user_question)\n",
    "            return {\"messages\": [HumanMessage(content=answer)]}\n",
    "    else:\n",
    "        return {\"messages\": [HumanMessage(content=\"No image provided for analysis.\")]}\n",
    "\n",
    "\n",
    "def meal_text_scan(state):\n",
    "    \"\"\"\n",
    "    Handle case when the question is deemed incorrect or irrelevant.\n",
    "    \"\"\"\n",
    "    print(\"---Text scan --\")\n",
    "    user_input = state[\"user_input\"]\n",
    "    user_question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    data = {\"meal_description\": user_question}\n",
    "    TOKEN = get_token(\n",
    "        \"https://api-dv.amwayglobal.com/rest/oauth2/v1/token\",\n",
    "        \"3hmyXKbHlA0ZLJ1Zjtg4G1X0l4srn0jIolK7pzB4EqiqBb1M\",\n",
    "        \"9Trey6amtSaRifSzU1HM2UlirkSLojkBCa0xWA51nUkyFeoGFFfVKWEuGdV8pNbu\",\n",
    "    )\n",
    "    headers_o = {\n",
    "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        \"x-hw-program\": \"mg_testing\",\n",
    "        \"x-abold\": \"mg_abo\",\n",
    "        \"x-mealtime\": \"\",\n",
    "        \"x-genai-vendor\": \"openai\",\n",
    "        \"x-country-code\": \"mg_testing\",\n",
    "    }\n",
    "    response = requests.post(\n",
    "        \"https://api-dv.amwayglobal.com/v1/health-wellbeing/mealanalyzer/meal-scan\",\n",
    "        headers=headers_o,\n",
    "        files={},\n",
    "        data=data,\n",
    "    )\n",
    "    answer = evaluate_api_response(state, response.json(), user_question)\n",
    "    return {\"messages\": [HumanMessage(content=answer)]}\n",
    "\n",
    "\n",
    "def barcode(state):\n",
    "    \"\"\"\n",
    "    Handle case when the question is deemed incorrect or irrelevant.\n",
    "    \"\"\"\n",
    "    print(\"---Barcode---\")\n",
    "    user_input = state[\"user_input\"]\n",
    "    user_question = state.get(\"corrected_question\", state[\"messages\"][0].content)\n",
    "    if user_input.image:\n",
    "        TOKEN = get_token(\n",
    "            \"https://api-dv.amwayglobal.com/rest/oauth2/v1/token\",\n",
    "            \"3hmyXKbHlA0ZLJ1Zjtg4G1X0l4srn0jIolK7pzB4EqiqBb1M\",\n",
    "            \"9Trey6amtSaRifSzU1HM2UlirkSLojkBCa0xWA51nUkyFeoGFFfVKWEuGdV8pNbu\",\n",
    "        )\n",
    "        headers_o = {\n",
    "            \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "            \"x-hw-program\": \"mg_testing\",\n",
    "            \"x-abold\": \"mg_abo\",\n",
    "            \"x-mealtime\": \"\",\n",
    "            \"x-genai-vendor\": \"openai\",\n",
    "        }\n",
    "        with open(user_input.image.filename, \"rb\") as img_file:\n",
    "            upc_file = {\"image\": img_file}\n",
    "            response = requests.post(\n",
    "                \"https://api-dv.amwayglobal.com/v1/health-wellbeing/mealanalyzer/upc\",\n",
    "                files=upc_file,\n",
    "                headers=headers_o,\n",
    "                data={},\n",
    "            )\n",
    "            answer = evaluate_api_response(state, response.json(), user_question)\n",
    "            return {\"messages\": [HumanMessage(content=answer)]}\n",
    "    else:\n",
    "        return {\"messages\": [HumanMessage(content=\"No image provided for analysis.\")]}\n",
    "\n",
    "\n",
    "PROJECT_ID = \"amw-dna-coe-working-ds-dev\"\n",
    "data_stores = [\n",
    "    {\"location\": \"global\", \"data_store_id\": \"amway-articles_1727879500677\"},\n",
    "    {\"location\": \"global\", \"data_store_id\": \"who-blog-unchunked_1728571550919\"},\n",
    "    {\"location\": \"global\", \"data_store_id\": \"demo-fbs-store_1729253761309\"},\n",
    "]\n",
    "\n",
    "\n",
    "class ChatManagerHandler:\n",
    "    def __init__(self):\n",
    "        self._managers = {}  # Dict to store managers for each user\n",
    "        # self._lock = Lock()\n",
    "        self._cleanup_interval = (\n",
    "            24 * 3600\n",
    "        )  # Cleanup interval in seconds (e.g., 24 hours)\n",
    "\n",
    "    def create_new_manager(self, user_id: str) -> \"EnhancedConversationManager\":\n",
    "        \"\"\"\n",
    "        Create a new manager for a specific user\n",
    "        \"\"\"\n",
    "        manager = EnhancedConversationManager(PROJECT_ID, data_stores)\n",
    "        self._managers[user_id] = {\n",
    "            \"manager\": manager,\n",
    "        }\n",
    "        print(\"created new manager\")\n",
    "        return manager\n",
    "\n",
    "    def get_current_manager(self, user_id: str) -> \"EnhancedConversationManager\":\n",
    "        \"\"\"\n",
    "        Get the manager for a specific user, creating one if it doesn't exist\n",
    "        \"\"\"\n",
    "        if user_id not in self._managers:\n",
    "            print(\"user id not found\")\n",
    "            return self.create_new_manager(user_id)\n",
    "\n",
    "        return self._managers[user_id][\"manager\"]\n",
    "\n",
    "\n",
    "# Global instance\n",
    "chat_manager_handler = ChatManagerHandler()\n",
    "manager = None\n",
    "\n",
    "\n",
    "def process_chatbot_request(\n",
    "    text: str, user_id: str, image_filename: Optional[str] = None\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\" \"\"\"\n",
    "    print(\"img name is\", image_filename)\n",
    "    image_input = ImageInput(filename=image_filename) if image_filename else None\n",
    "    print(image_input)\n",
    "    user_input = UserInput(text=text, image=image_input)\n",
    "    print(user_input)\n",
    "    initial_state = {\n",
    "        \"user_input\": user_input,\n",
    "        \"messages\": [],\n",
    "        \"retrieved_docs\": \"\",\n",
    "    }\n",
    "\n",
    "    final_output = \"\"\n",
    "    print(\"user id in chatbot api is\", user_id)\n",
    "    manager = chat_manager_handler.get_current_manager(user_id)\n",
    "    final_output = manager.process_question(user_input, user_id)\n",
    "    return final_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">user id not found\n",
       "</pre>\n"
      ],
      "text/plain": [
       "user id not found\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Starting unified initialization\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Starting unified initialization\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed unified initialization\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed unified initialization\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">created new manager\n",
       "</pre>\n"
      ],
      "text/plain": [
       "created new manager\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">__main__.EnhancedConversationManager</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x3068218e0</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95m__main__.EnhancedConversationManager\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x3068218e0\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manager = chat_manager_handler.get_current_manager(1)\n",
    "print(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display graph visualization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[0;32m----> 3\u001b[0m graph_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()\n\u001b[1;32m      4\u001b[0m display(Image(graph_image))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = UserInput(text=\" tell me about nutrilite protein powder  \", image=None)\n",
    "\n",
    "final_output = manager.process_question(user_input,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_query(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SQL query by removing code block syntax, various SQL tags, backticks,\n",
    "    prefixes, and unnecessary whitespace while preserving the core SQL query.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw SQL query text that may contain code blocks, tags, and backticks\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned SQL query\n",
    "    \"\"\"\n",
    "    # Step 1: Remove code block syntax and any SQL-related tags\n",
    "    # This handles variations like ```sql, ```SQL, ```SQLQuery, etc.\n",
    "    block_pattern = r\"```(?:sql|SQL|SQLQuery|mysql|postgresql)?\\s*(.*?)\\s*```\"\n",
    "    text = re.sub(block_pattern, r\"\\1\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Step 2: Handle \"SQLQuery:\" prefix and similar variations\n",
    "    # This will match patterns like \"SQLQuery:\", \"SQL Query:\", \"MySQL:\", etc.\n",
    "    prefix_pattern = r\"^(?:SQL\\s*Query|SQLQuery|MySQL|PostgreSQL|SQL)\\s*:\\s*\"\n",
    "    text = re.sub(prefix_pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 3: Extract the first SQL statement if there's random text after it\n",
    "    # Look for a complete SQL statement ending with semicolon\n",
    "    sql_statement_pattern = r\"(SELECT.*?;)\"\n",
    "    sql_match = re.search(sql_statement_pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if sql_match:\n",
    "        text = sql_match.group(1)\n",
    "\n",
    "    # Step 4: Remove backticks around identifiers\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Step 5: Normalize whitespace\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Step 6: Preserve newlines for main SQL keywords to maintain readability\n",
    "    keywords = ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'HAVING', 'ORDER BY',\n",
    "               'LIMIT', 'JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'INNER JOIN',\n",
    "               'OUTER JOIN', 'UNION', 'VALUES', 'INSERT', 'UPDATE', 'DELETE']\n",
    "\n",
    "    # Case-insensitive replacement for keywords\n",
    "    pattern = '|'.join(r'\\b{}\\b'.format(k) for k in keywords)\n",
    "    text = re.sub(f'({pattern})', r'\\n\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 7: Final cleanup\n",
    "    # Remove leading/trailing whitespace and extra newlines\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigquery_schema(project_id, dataset_id, table_id):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # Get the full table reference\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    \n",
    "    # Retrieve the table\n",
    "    table = client.get_table(table_ref)\n",
    "    \n",
    "    schema_details = []\n",
    "    for field in table.schema:\n",
    "        schema_details.append({\n",
    "            'name': field.name,\n",
    "            'type': field.field_type,\n",
    "            'mode': field.mode,\n",
    "        })\n",
    "    \n",
    "    return schema_details \n",
    "\n",
    "def execute_bigquery_query(query: str) -> str:\n",
    "    \"\"\"Execute BigQuery query and return results as formatted string\"\"\"\n",
    "    try:\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(query)\n",
    "        results = query_job.result()\n",
    "        \n",
    "        # Convert results to DataFrame and then to string\n",
    "        df = results.to_dataframe()\n",
    "        if len(df) > 10:  # Limit large results\n",
    "            df = df.head(10)\n",
    "        return df.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error executing query: {str(e)}\"\n",
    "\n",
    "def generate_bigquery_query2(schema: str, question: str) -> str:\n",
    "    \"\"\"Generate BigQuery SQL query from natural language question\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a data analyst who converts natural language questions into BigQuery SQL queries.\n",
    "        Using the following schema, write a SQL query to answer the user's question.\n",
    "        \n",
    "        SCHEMA:\n",
    "        {schema}\n",
    "        \n",
    "        QUESTION: {question}\n",
    "        \n",
    "        Write only the SQL query, nothing else. Ensure the query is compatible with BigQuery SQL syntax.\n",
    "        \"\"\",\n",
    "        input_variables=[\"schema\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    return chain.invoke({\"schema\": schema, \"question\": question})\n",
    "\n",
    "def generate_bigquery_query(schema: str, question: str, state: Dict = None) -> str:\n",
    "    \"\"\"Generate BigQuery SQL query from natural language question\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    # Get user_id/abo_id from state if available\n",
    "    abo_id = state.get(\"user_id\") if state else None\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a data analyst who converts natural language questions into BigQuery SQL queries.\n",
    "        \n",
    "        IMPORTANT CONTEXT:\n",
    "        - The table is located at: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        - Current ABO ID (if needed): {abo_id}\n",
    "        \n",
    "        SCHEMA:\n",
    "        {schema}\n",
    "        \n",
    "        QUESTION: {question}\n",
    "        \n",
    "        QUERY GUIDELINES:\n",
    "        1. Always use the full table path: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        2. If the question implies personal data or \"my\" information, use the ABO ID filter\n",
    "        3. For general queries, don't include ABO ID filter\n",
    "        4. Always include appropriate LIMIT clause for large result sets\n",
    "        5. Use clear column aliases for better readability\n",
    "        \n",
    "        Examples:\n",
    "        - \"Show my affiliate ID\"\n",
    "        ```sql\n",
    "        SELECT aff_id \n",
    "        FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        WHERE global_account_id = {abo_id}\n",
    "        LIMIT 1\n",
    "        ```\n",
    "        \n",
    "        - \"Count all users\"\n",
    "        ```sql\n",
    "        SELECT COUNT(DISTINCT global_account_id) as total_users\n",
    "        FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        ```\n",
    "        \n",
    "        Write only the SQL query, nothing else. Ensure it's a valid BigQuery SQL query.\n",
    "        \"\"\",\n",
    "        input_variables=[\"schema\", \"question\", \"abo_id\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    ans =  chain.invoke({\n",
    "        \"schema\": schema, \n",
    "        \"question\": question,\n",
    "        \"abo_id\": abo_id if abo_id else \"NULL\"\n",
    "    })\n",
    "    return clean_sql_query(ans)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"amw-dna-coe-working-ds-dev\"\n",
    "dataset_id = \"data_science\"\n",
    "table_id = \"abo_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheman = get_bigquery_schema(project_id,dataset_id,table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general query\n",
    "state = {\"user_id\": 1007023167859} \n",
    "query = generate_bigquery_query(scheman, \"my total volume this year\", state)\n",
    "# Should output something like:\n",
    "# SELECT COUNT(DISTINCT global_account_id) as total_users\n",
    "# FROM `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "print(query)\n",
    "# For personal query with state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = execute_bigquery_query(query)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_execute_query(schema: str, question: str, state: Dict = None) -> str:\n",
    "    \"\"\"Generate BigQuery SQL query, execute it, and provide natural language response\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "    \n",
    "    # Get user_id/abo_id from state if available\n",
    "    abo_id = state.get(\"user_id\") if state else None\n",
    "    \n",
    "    # SQL Generation Prompt\n",
    "    sql_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a data analyst who converts natural language questions into BigQuery SQL queries.\n",
    "        \n",
    "        IMPORTANT CONTEXT:\n",
    "        - The table is located at: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        - Current ABO ID (if needed): {abo_id}\n",
    "        \n",
    "        SCHEMA:\n",
    "        {schema}\n",
    "        \n",
    "        QUESTION: {question}\n",
    "        \n",
    "        QUERY GUIDELINES:\n",
    "        1. Always use the full table path: `amw-dna-coe-working-ds-dev.data_science.abo_info`\n",
    "        2. If the question implies personal data or \"my\" information, use the ABO ID filter\n",
    "        3. For general queries, don't include ABO ID filter\n",
    "        4. Always include appropriate LIMIT clause for large result sets\n",
    "        5. Use clear column aliases for better readability\n",
    "        \n",
    "        Write only the SQL query, nothing else. Ensure it's a valid BigQuery SQL query.\n",
    "        \"\"\",\n",
    "        input_variables=[\"schema\", \"question\", \"abo_id\"]\n",
    "    )\n",
    "    \n",
    "    # Answer Generation Prompt\n",
    "    answer_prompt = PromptTemplate(\n",
    "        template=\"\"\"Given the following user question, SQL query, and query result, provide a natural language answer.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "Query Result: {result}\n",
    "\n",
    "Guidelines for answer:\n",
    "1. Be conversational and friendly\n",
    "2. Explain the results clearly\n",
    "3. If the query is personal (uses ABO ID), make the response personal using \"your\" instead of \"the\"\n",
    "4. Include relevant numbers/statistics from the result\n",
    "5. If the result is empty or null, explain that no data was found\n",
    "\n",
    "Answer: \"\"\",\n",
    "        input_variables=[\"question\", \"query\", \"result\"]\n",
    "    )\n",
    "    \n",
    "    # Create chains\n",
    "    sql_chain = sql_prompt | model | StrOutputParser()\n",
    "    answer_chain = answer_prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Generate SQL query\n",
    "    query = sql_chain.invoke({\n",
    "        \"schema\": schema,\n",
    "        \"question\": question,\n",
    "        \"abo_id\": abo_id if abo_id else \"NULL\"\n",
    "    })\n",
    "    \n",
    "    # Clean the SQL query\n",
    "    clean_query = clean_sql_query(query)\n",
    "    \n",
    "    # Execute query\n",
    "    result = execute_bigquery_query(clean_query)\n",
    "    \n",
    "    # Generate natural language answer\n",
    "    answer = answer_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"query\": clean_query,\n",
    "        \"result\": result\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"query\": clean_query,\n",
    "        \"result\": result,\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mannugaddhyan/Downloads/dpds-health-wellness-api/mannu2/lib/python3.12/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SQL Query: SELECT <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SUM</span><span style=\"font-weight: bold\">(</span>current_year_avg_total_downline_pv_normalized_to_10k<span style=\"font-weight: bold\">)</span> AS total_sales_this_year \n",
       "FROM amw-dna-coe-working-ds-dev.data_science.abo_info \n",
       "LIMIT <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>;\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SQL Query: SELECT \u001b[1;35mSUM\u001b[0m\u001b[1m(\u001b[0mcurrent_year_avg_total_downline_pv_normalized_to_10k\u001b[1m)\u001b[0m AS total_sales_this_year \n",
       "FROM amw-dna-coe-working-ds-dev.data_science.abo_info \n",
       "LIMIT \u001b[1;36m1\u001b[0m;\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Query Result:   total_sales_this_year\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1622310059.000000000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Query Result:   total_sales_this_year\n",
       "\u001b[1;36m0\u001b[0m  \u001b[1;36m1622310059.000000000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "Natural Language Answer: Hey there! So, you were curious about the total sales for this year, right? Well, I've got\n",
       "some good news for you. After crunching the numbers, the total sales for this year came out to be a whopping \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">622</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">310</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">059</span>! That's over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6</span> billion when you round it up. Pretty impressive, isn't it? If you have any more \n",
       "questions or need further breakdowns, feel free to ask. Happy to help!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "Natural Language Answer: Hey there! So, you were curious about the total sales for this year, right? Well, I've got\n",
       "some good news for you. After crunching the numbers, the total sales for this year came out to be a whopping \n",
       "\u001b[1;36m1\u001b[0m,\u001b[1;36m622\u001b[0m,\u001b[1;36m310\u001b[0m,\u001b[1;36m059\u001b[0m! That's over \u001b[1;36m1.6\u001b[0m billion when you round it up. Pretty impressive, isn't it? If you have any more \n",
       "questions or need further breakdowns, feel free to ask. Happy to help!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage with state\n",
    "state = {\"user_id\": 1007022633276}\n",
    "response = generate_and_execute_query(\n",
    "    schema=scheman,\n",
    "    question=\" total sales this year? \",\n",
    "    state=state\n",
    ")\n",
    "\n",
    "print(\"SQL Query:\", response[\"query\"])\n",
    "print(\"\\nQuery Result:\", response[\"result\"])\n",
    "print(\"\\nNatural Language Answer:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display graph visualization\n",
    "    from IPython.display import Image, display\n",
    "    graph_image = self.graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mannu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
